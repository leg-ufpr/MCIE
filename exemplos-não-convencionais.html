<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Exemplos não convencionais | Exemplos2.utf8</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Exemplos não convencionais | Exemplos2.utf8" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Exemplos não convencionais | Exemplos2.utf8" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="config/rmcd.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<center><li><strong><a href="./">
  Statistical Inference
</a></strong></li></center>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path=""><a href="#exemplos-não-convencionais"><i class="fa fa-check"></i><b>1</b> Exemplos não convencionais</a><ul>
<li class="chapter" data-level="1.1" data-path=""><a href="#distribuição-gaussiana"><i class="fa fa-check"></i><b>1.1</b> Distribuição Gaussiana</a><ul>
<li class="chapter" data-level="1.1.1" data-path=""><a href="#dados-intervalares"><i class="fa fa-check"></i><b>1.1.1</b> Dados intervalares</a></li>
<li class="chapter" data-level="1.1.2" data-path=""><a href="#informação-de-cada-dado"><i class="fa fa-check"></i><b>1.1.2</b> Informação de cada dado</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path=""><a href="#distribuição-gama"><i class="fa fa-check"></i><b>1.2</b> Distribuição Gama</a><ul>
<li class="chapter" data-level="1.2.1" data-path=""><a href="#parametrizações-para-gama"><i class="fa fa-check"></i><b>1.2.1</b> Parametrizações para Gama</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path=""><a href="#distribuição-binomial-negativa"><i class="fa fa-check"></i><b>1.3</b> Distribuição Binomial Negativa</a></li>
</ul></li>
<li class="divider"></li>
</hr>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:title:end-->
<!--bookdown:title:start-->
<div id="exemplos-não-convencionais" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Exemplos não convencionais</h1>
<div id="distribuição-gaussiana" class="section level2">
<h2><span class="header-section-number">1.1</span> Distribuição Gaussiana</h2>

<div class="example">
<span id="exm:vero-normal" class="example"><strong>Example 1.1  </strong></span><strong>Exemplo - Distribuição Gaussiana</strong>
</div>

<p>Suponha que <span class="math inline">\(Y_1, Y_2, \ldots, Y_n\)</span> são variáveis aleatórias independentes e identicamente distribuídas com distribuição gaussiana de média <span class="math inline">\(\mu\)</span> e variância <span class="math inline">\(\sigma^2\)</span>. Denote isto por <span class="math inline">\(Y_i \sim N(\mu, \sigma^2)\)</span>. Note que neste caso o vetor de parâmetros é <span class="math inline">\(\underline{\theta} = (\mu, \sigma)^{\top}\)</span>, onde <span class="math inline">\(\mu \in \Re\)</span> e <span class="math inline">\(\sigma \in \Re^+\)</span> são os respectivos espaços paramétricos. O objetivo é estimar <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma\)</span> além de encontrar intervalos ou regiões de confiança. Note que agora temos dois parâmetros e a função de log-verossimilhança é uma superfície. Os princípios vistos no caso uniparamétrico se mantém, mas
a construção de gráficos e a obtenção das estimativas são mais trabalhosas. Vejamos alguns fatos relevantes deste exemplo. Como sempre, começamos escrevendo a função de verossimilhança,
<span class="math display">\[
L(\mu, \sigma) = (2\pi)^{-n/2} \sigma^{-n} \exp\{ - \frac{1}{2 \sigma^2} \sum_{i=1}^n (y_i - \mu)^2\}.
\]</span>
A log-verossimilhança é dada por,
<span class="math display">\[
l(\mu,\sigma) = -\frac{n}{2} \log 2 \pi - n \log \sigma - \frac{1}{2 \sigma^2} \sum_{i=1}^n ( y_i - \mu)^2.
\]</span>
A função  toma a forma de um sistema de equações,
<span class="math display">\[\begin{align*}
U(\mu) &amp;= \frac{\partial l(\mu, \sigma)}{\partial \mu} = \frac{\sum_{i=1}^n y_i }{\sigma^2} - \frac{n \mu}{\sigma^2} \\
U(\sigma) &amp;= -\frac{n}{\sigma} + \frac{1}{\sigma^3} \sum_{i=1}^n (y_i - \mu)^2 .
\end{align*}\]</span>
Neste caso podemos facilmente resolver este sistema chegando as estimativas de máxima verossimilhança,
<span class="math display">\[
  \hat{\mu} = \frac{\sum_{i=1}^n y_i}{n} \quad \text{e} \quad \hat{\sigma}^2 = \frac{\sum_{i=1}^n (y_i - \mu)^2}{n}.
\]</span>
A matriz de informação observada fica da seguinte forma,
<span class="math display">\[
I_O (\mu, \sigma) = \left[\begin{array}{cc}
 -\frac{\partial^2 l(\mu, \sigma)}{\partial \mu^2}  &amp; - \frac{\partial^2 l(\mu, \sigma)}{\partial \mu \partial \sigma}  \\
  - \frac{\partial^2 l(\mu, \sigma)}{\partial \mu \partial \sigma} &amp; -\frac{\partial^2 l(\mu, \sigma)}{\partial \sigma^2}
\end{array}\right] .
\]</span>
Temos então,
<span class="math display">\[\begin{align*}
  \frac{\partial^2 l(\mu, \sigma)}{\partial \mu^2} &amp;= \frac{\partial U(\mu)}{\partial \mu} = - \frac{n}{\sigma^2} \\
  \frac{\partial^2 l(\mu, \sigma)}{\partial \sigma^2} &amp;= \frac{\partial U(\sigma)}{\partial \sigma} = - \frac{2n}{\sigma^2} \\
  \frac{\partial^2 l(\mu, \sigma)}{\partial \mu \partial \sigma} &amp;= \frac{\partial U(\sigma)}{\partial \sigma} = -\frac{2}{\sigma^3}
  \sum_{i=1}^n (y_i - \overline{y}) = 0.
\end{align*}\]</span>
Logo,
<span class="math display">\[ I_O(\hat{\mu}, \hat{\sigma}) = \left[\begin{array}{cc}
    \frac{n}{\hat{\sigma}^2} &amp; 0 \\
    0 &amp; \frac{2n}{ \hat{\sigma}^2}  
\end{array}\right] .
\]</span></p>
<p>Neste caso a matriz de informação observada coincide com a matriz de informação esperada. Além disso, note a importante propriedade de ortogonalidade entre os parâmetros, indicada pelos termos fora da diagonal da matriz de informação serem zero. A derivada cruzada entre dois parâmetros ser zero, é condição suficiente para que estes parâmetros sejam ortogonais. A ortogonalidade é uma propriedade muito conveniente e simplifica as inferências, uma vez que podemos fazer inferência para um parâmetro sem nos preocupar com os valores do outro.</p>
<p>Para construção dos intervalos de confiança, a maneira mais direta é usar os resultados assintóticos dos estimadores de máxima verossimilhança, neste caso temos que a distribuição assintótica de <span class="math inline">\(\hat{\underline{\theta}} = (\hat{\mu}, \hat{\sigma})^\top\)</span> é
<span class="math display">\[
\begin{bmatrix}
\hat{\mu}  \\ \hat{\sigma}
\end{bmatrix} \sim NM_2\left (\begin{bmatrix}
\mu  \\ \sigma
\end{bmatrix} , \begin{bmatrix}
\hat{\sigma}^2/n &amp; 0 \\ 
 0 &amp; \hat{\sigma}^2/2n 
\end{bmatrix} \right )
\]</span>
Intervalos de confiança de Wald podem ser obtidos por:
<span class="math display">\[
  \hat{\mu} \pm z_{\alpha/2} \sqrt{\hat{\sigma}^2/n} 
\]</span>
e para <span class="math inline">\(\sigma\)</span> temos
<span class="math display">\[
\hat{\sigma} \pm z_{\alpha/2} \sqrt{\hat{\sigma}^2/2n}.
\]</span></p>
<p>A função de verossimilhança deste exemplo é simétrica e quadrática na direção de <span class="math inline">\(\mu\)</span>,
e portanto a aproximação quadrática coincide com a forma exata na direção deste parâmetro.
Porém, a verossimilhança é assimétrica na direção de <span class="math inline">\(\sigma\)</span>.
Destacamos ainda que a assimetria é maior <span class="math inline">\(\sigma^2\)</span>, um pouco menos acentuada em <span class="math inline">\(\sigma\)</span>
e ainda menos acentuada em uma transformação não linear como <span class="math inline">\(\psi = \log(\sigma)\)</span>.
Nos remetemos à discussão na Sessão <a href="#sec:vero-repar"><strong>??</strong></a> para mais detalhes e implicações.
Na prática, se intervalos baseados na aproximação quadrática serão utilizados
(por vezes a partir de hessianos numéricos),
o mais recomendado então é reparametrizar a verossimilhança como função de <span class="math inline">\(\psi\)</span>
para obter uma forma mais próxima à simetria.
Pode-se então obter intervalos assintóticos para <span class="math inline">\(\psi\)</span> e depois transformá-los para escala original do parâmetro, por transformação direta se verossimilhança em <span class="math inline">\(\psi\)</span> for muito próxima à simetria
ou, caso contrário, pelo método delta.</p>
<p>Outra opção é obter uma região de confiança baseada na ,
<span class="math display">\[\begin{align*}
  D(\mu, \sigma) &amp;= 2 [ l(\hat{\mu}, \hat{\sigma}) - l(\mu,\sigma)] \\
                 &amp;= 2[ n \log \left( \frac{\sigma}{\hat{\sigma}} \right) + \frac{1}{2\sigma^2}\sum_{i=1}^n(y_i - \mu)^2 - \frac{1}{2\hat{\sigma}^2} \sum_{i=1}^n (y_i - \hat{\mu})].
\end{align*}\]</span>
A  aproximada tem a seguinte forma
<span class="math display">\[
  D(\mu, \sigma) \approx ( \underline{\theta} - \underline{\hat{\theta}})^\top I_o(\underline{\hat{\theta}}) ( \underline{\theta} - \underline{\hat{\theta}}).
\]</span></p>
<p>Note que neste caso a superfície de log-verossimilhança em duas dimensões esta sendo aproximada por uma elipse. É bastante intuitivo pensar que aproximar uma função em duas dimensões é mais difícil que em uma.
Sabemos também que esta aproximação tenderá a ser melhor quanto maior for o tamanho da amostra.
Para exemplificar esta ideia a Figura <a href="#fig:devNormal">1.1</a> apresenta o gráfico da função  bidimensional em <span class="math inline">\((\mu, \sigma)\)</span> para o caso do modelo gaussiano para quatro tamanhos de amostra, n = 10, 50, 100 e 1000.</p>
<div class="figure"><span id="fig:devNormal"></span>
<img src="Exemplos2_files/figure-html/devNormal-1.png" alt="Deviance exata (linha sólida) e aproximada (linha tracejada) para diferentes tamanhos de amostra - Distribuição Normal." width="480" />
<p class="caption">
Figura 1.1: Deviance exata (linha sólida) e aproximada (linha tracejada) para diferentes tamanhos de amostra - Distribuição Normal.
</p>
</div>
<p>Na Figura <a href="#fig:devNormal">1.1</a> vemos que com <span class="math inline">\(n=10\)</span>
a verossimilhança exibe forte assimetria na direção do parâmetro <span class="math inline">\(\sigma\)</span> e
a aproximação quadrática é claramente insatisfatória.
Com o aumento do tamanho da amostra a aproximação quadrática vai ficando cada vez mais próxima da  exata, mais uma vez ilustrando o comportamento assintótico da verossimilhança.
É importante notar também que em modelos com dois ou mais parâmetros a aproximação pode melhorar
mais rapidamente para um do que para o outro parâmetro.
No exemplo a aproximação é exata para <span class="math inline">\(\mu\)</span> para qualquer tamanho de amostra.
Já para <span class="math inline">\(\sigma\)</span> é necessário um tamanho de amostra relativamente grande
para que a  em sua direção tome um comportamento próximo do simétrico.
A função se aproxima da simetria mais rapidamente se parametrizada com <span class="math inline">\(\log(\sigma)\)</span>.</p>
<p>Em outros modelos, como no caso dos modelos lineares generalizados (MLG) a intuição é a mesma ainda que a aproximação para parâmetros de média deixe de ser exata.
De forma geral, para parâmetros de média a aproximação quadrática
tende a apresentar bons resultados mesmo com amostras reduzidas.
O mesmo não pode ser dito para parâmetros de dispersão ou mesmo de correlação.
Em outras palavras, estimar a média é mais simples que estimar a variabilidade,
que por sua vez é mais simples do que estimar correlações.</p>
<p>As regiões de confiança são as curvas de nível na superfície de . Note que, apenas com a  não temos intervalos marginais como os obtidos pela aproximação quadrática.
Uma possível solução é projetar a superfície na direção do parâmetro de interesse na maior amplitude, que é obtida quando fixamos o outro parâmetro na sua estimativa de máxima verossimilhança.
Porém esta prática só produz bons resultados quando os parâmetros são ao menos aproximadamente ortogonais.
Uma solução mais genérica, ainda que computacionalmente mais trabalhosa é o obtenção das verossimilhanças
perfilhadas.<br />
No caso particular da distribuição gaussiana, que tem a propriedade de ortogonalidade entre <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma\)</span>, a verossimilhança condicionada na estimativa de máxima verossimilhança coincide com a verossimilhança perfilhada.
Para ilustrar este fato considere a obtenção da verossimilhança perfilhada para <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma\)</span>
pelas funções a seguir:</p>

<div class="lemma">
<span id="lem:unnamed-chunk-2" class="lemma"><strong>Código 1.1  </strong></span><strong>Função para log-verossimilhança perfilhada dos parâmetros <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma\)</span> da distribuição gaussiana.</strong>
</div>

<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1">pl.mu &lt;-<span class="st"> </span><span class="cf">function</span>(sigma, mu, dados){</a>
<a class="sourceLine" id="cb1-2" data-line-number="2">    pll &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">dnorm</span>(dados, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sigma, <span class="dt">log=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb1-3" data-line-number="3">    <span class="kw">return</span>(pll)}</a>
<a class="sourceLine" id="cb1-4" data-line-number="4"><span class="co">## Perfil para sigma</span></a>
<a class="sourceLine" id="cb1-5" data-line-number="5">pl.sigma &lt;-<span class="st"> </span><span class="cf">function</span>(mu, sigma, dados){</a>
<a class="sourceLine" id="cb1-6" data-line-number="6">    pll &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">dnorm</span>(dados, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sigma, <span class="dt">log=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb1-7" data-line-number="7">    <span class="kw">return</span>(pll)}</a></code></pre></div>
<p>Vamos criar uma malha de valores nos quais a função será avaliada para a construção dos gráficos.
Também vamos obter a log-verossimilhança condicionada na estimativa de máxima verossimilhança,
que consiste em avaliar apenas a função para um dos parâmetros com o outro fixado em sua estimativa.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb2-2" data-line-number="2">y10 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">10</span>,<span class="dv">10</span>,<span class="fl">1.5</span>)</a>
<a class="sourceLine" id="cb2-3" data-line-number="3">grid.mu &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">9</span>, <span class="fl">11.3</span>, <span class="dt">length=</span><span class="dv">200</span>)</a>
<a class="sourceLine" id="cb2-4" data-line-number="4">grid.sigma &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.65</span>, <span class="fl">2.7</span>, <span class="dt">length=</span><span class="dv">200</span>)</a>
<a class="sourceLine" id="cb2-5" data-line-number="5"><span class="co">## Condicional para mu:</span></a>
<a class="sourceLine" id="cb2-6" data-line-number="6">mu.cond &lt;-<span class="st"> </span><span class="kw">sapply</span>(grid.mu, pl.sigma, <span class="dt">sigma=</span><span class="kw">sqrt</span>(<span class="kw">var</span>(y10)<span class="op">*</span><span class="dv">9</span><span class="op">/</span><span class="dv">10</span>), <span class="dt">dados=</span>y10)</a>
<a class="sourceLine" id="cb2-7" data-line-number="7"><span class="co">## Condicional para sigma:</span></a>
<a class="sourceLine" id="cb2-8" data-line-number="8">sigma.cond &lt;-<span class="st"> </span><span class="kw">sapply</span>(grid.sigma, pl.mu, <span class="dt">mu=</span><span class="kw">mean</span>(y10), <span class="dt">dados=</span>y10)</a></code></pre></div>
<p>Para obter o perfil de verossimilhança, por exemplo para <span class="math inline">\(\sigma\)</span> precisamos de uma malha de valores de <span class="math inline">\(\sigma\)</span> e para cada valor nesta malha encontrar o valor digamos <span class="math inline">\(\hat{\mu}_{\sigma}\)</span> que maximiza a verossimilhança perfilhada.
Para este exemplo existem formas fechadas para os estimadores, portanto basta aplicar
a expressão do estimador de um parâmetro para cada valor na malha de valores do parâmetro sendo perfilhando.
Entretanto para ilustração utilizamos uma forma mais geral,
adequada para casos onde não há expressões fechadas, na qual maximizamos a função utilizando
procedimentos numéricos, o que implica em uma otimização numérica de um parâmetro para cada
valor na grade do parâmetro que está sendo perfilhado.
Para a maximização usamos a função  própria para maximização em apenas uma dimensão como é o caso neste exemplo. O código abaixo ilustra o procedimento para o conjunto de 10 dados.
O gráfico da esquerda da Figura <a href="#fig:devcondperf">1.2</a> mostra a superfície de deviance com
as linhas tracejadas indicando os cortes para obtenção das deviances perfilhadas
dos gráficos do centro e a direita. Nestes gráficos são também mostradas as
funções deviance condicionadas no MLE (linha sólida).</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1">mu.perf &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span><span class="kw">length</span>(mu), <span class="dt">ncol=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb3-2" data-line-number="2"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(mu)){</a>
<a class="sourceLine" id="cb3-3" data-line-number="3">mu.perf[i,] &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">optimize</span>(pl.mu,<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">200</span>), </a>
<a class="sourceLine" id="cb3-4" data-line-number="4">                 <span class="dt">mu=</span>mu[i],<span class="dt">dados=</span>y10,<span class="dt">maximum=</span><span class="ot">TRUE</span>))}</a>
<a class="sourceLine" id="cb3-5" data-line-number="5"></a>
<a class="sourceLine" id="cb3-6" data-line-number="6">sigma.perf &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span><span class="kw">length</span>(sigma), <span class="dt">ncol=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb3-7" data-line-number="7"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(sigma)){</a>
<a class="sourceLine" id="cb3-8" data-line-number="8">sigma.perf[i,] &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">optimize</span>(pl.sigma,<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1000</span>), </a>
<a class="sourceLine" id="cb3-9" data-line-number="9">                <span class="dt">sigma=</span>sigma[i],<span class="dt">dados=</span>y10,<span class="dt">maximum=</span><span class="ot">TRUE</span>))}</a></code></pre></div>
<div class="figure"><span id="fig:devcondperf"></span>
<img src="Exemplos2_files/figure-html/devcondperf-1.png" alt="Deviance conjunta, perfilhada e condicional para $\mu$ e $\sigma$ - Distribuição Normal." width="672" />
<p class="caption">
Figura 1.2: Deviance conjunta, perfilhada e condicional para <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma\)</span> - Distribuição Normal.
</p>
</div>
<p>Figura <a href="#fig:devcondperf">1.2</a> ilustra que a  perfilhada
e a  condicional coincidem para o parâmetro <span class="math inline">\(\sigma\)</span>
porém não para o parâmetro de média <span class="math inline">\(\mu\)</span>. Isto reflete o fato de que ao perfilhar <span class="math inline">\(\sigma\)</span>
o valor maximizado <span class="math inline">\(\hat{\mu}_{\sigma} = \hat{\mu}\)</span> e não depende de <span class="math inline">\(\sigma\)</span>.
Já no perfil de <span class="math inline">\(\mu\)</span> cada um dos valores maximizados <span class="math inline">\(\hat{\sigma}_{\mu}\)</span> dependem dos valores de <span class="math inline">\(\mu\)</span>.
Para obter intervalos de confiança basta definir o corte nestas funções seja por valor relativo da verossimilhança
ou usando o quantil da distribuição <span class="math inline">\(\chi^2\)</span> para o nível de confiança desejado e encontrar as raízes da equação,
assim como nos exemplos uniparamétricos.
A verossimilhança perfilhada permite tratar um problema multiparamétrico como um problema uniparamétrico
levando em consideração a forma da verossimilhança na direção de todos os parâmetros do modelo.
Porém, esta abordagem pode ser extremamente cara computacionalmente,
uma vez que para cada avaliação da verossimilhança perfilhada pode ser necessário uma maximização, que em geral vai requerer algum método numérico.</p>
<div id="dados-intervalares" class="section level3">
<h3><span class="header-section-number">1.1.1</span> Dados intervalares</h3>

<div class="example">
<span id="exm:normalCens2EX" class="example"><strong>Example 1.2  </strong></span><strong>Exemplo - Dados intervalares</strong>
</div>

<p>Quando definimos a função de verossimilhança no início do livro,
mencionamos que os dados são medidos em algum intervalo
definido pela precisão da medição.
No exemplo anterior fizemos a suposição usual de que
este intervalo é pequeno em relação a variação dos dados
e portanto os valores dos dados são tratados como pontos
na cálculo da função de verossimilhança
e utilizamos <a href="#eq:veroiid">(<strong>??</strong>)</a></p>
<p>Vamos considerar agora a situação na qual os dados
são medidos em intervalos .
Desta forma voltamos a definição mais geral da verossimilhança
em <a href="#eq:verogeral">(<strong>??</strong>)</a> para obter sua expressão.</p>
Como exemplo vamos considerar a distribuição gaussiana <span class="math inline">\(Y_i \sim N(\mu, \sigma^2)\)</span>,
<span class="math inline">\(\underline{\theta} = (\mu, \sigma)\)</span>.
Suponha que temos um conjunto de dados
que consiste de:

<p>Supondo independência, a contribuição para verossimilhança das observações
pontuais é o valor da densidade no ponto,
enquanto que para as intervalares é a probabilidade
da observação estar no intervalo.
Para os tipos de dados neste exemplo temos:
\begin{flalign<em>}
L() &amp;= f(y_i)  y_i , \
L() &amp;= 1 - F(85)  y_i &gt; 85, \
L() &amp;= 1 - F(80)  y_i &gt; 80, \
L() &amp;= F(80) - F(75)  75 &lt; y_i &lt; 80,\
L() &amp;= F(75)  y_i &lt; 85 .
\end{flalign</em>}</p>
<p>A seguir escrevemos a função de (negativo da) verossimilhança
que recebe como argumentos os parâmetros,
os dados pontuais como um vetor e os intervalares
como uma matriz de duas colunas na qual cada linha corresponde a um dado.</p>

<div class="lemma">
<span id="lem:unnamed-chunk-6" class="lemma"><strong>Código 1.2  </strong></span><strong>Função para log-verossimilhança para dados pontuais e intervalares
de distribuição gaussiana.</strong>
</div>

<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1">nllnormI &lt;-<span class="st"> </span><span class="cf">function</span>(par, yp, YI){</a>
<a class="sourceLine" id="cb4-2" data-line-number="2">    <span class="co">## yp : vetor com observações &quot;pontuais&quot;</span></a>
<a class="sourceLine" id="cb4-3" data-line-number="3">    <span class="co">## XI: matrix (n x 2) com dados intervalares </span></a>
<a class="sourceLine" id="cb4-4" data-line-number="4">    ll1 &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">dnorm</span>(yp, <span class="dt">mean=</span>par[<span class="dv">1</span>], <span class="dt">sd=</span>par[<span class="dv">2</span>], <span class="dt">log=</span>T))</a>
<a class="sourceLine" id="cb4-5" data-line-number="5">    L2 &lt;-<span class="st"> </span><span class="kw">pnorm</span>(YI, <span class="dt">mean=</span>par[<span class="dv">1</span>], <span class="dt">sd=</span>par[<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb4-6" data-line-number="6">    ll2 &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">log</span>(L2[,<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>L2[,<span class="dv">1</span>]))</a>
<a class="sourceLine" id="cb4-7" data-line-number="7">    <span class="kw">return</span>(<span class="op">-</span>(ll1 <span class="op">+</span><span class="st"> </span>ll2))</a>
<a class="sourceLine" id="cb4-8" data-line-number="8">}</a></code></pre></div>
<p>Nos comandos a seguir definimos os objetos que contém os dados.
A matriz dos dados intervalares é transposta apenas para visualização. Usamos estimativas baseadas nos dados completos como
valores iniciais e encontramos as estimativas usando todos os
dados maximizando e função de verossimilhança numericamente.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">72.6</span>, <span class="fl">81.3</span>, <span class="fl">72.4</span>, <span class="fl">86.4</span>, <span class="fl">79.2</span>, <span class="fl">76.7</span>, <span class="fl">81.3</span>)        </a>
<a class="sourceLine" id="cb5-2" data-line-number="2"><span class="kw">t</span>(xI &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">80</span>, <span class="kw">rep</span>(<span class="dv">75</span>, <span class="dv">4</span>), <span class="kw">rep</span>(<span class="op">-</span><span class="ot">Inf</span>, <span class="dv">6</span>)),</a>
<a class="sourceLine" id="cb5-3" data-line-number="3">              <span class="kw">c</span>(<span class="kw">rep</span>(<span class="ot">Inf</span>, <span class="dv">2</span>), <span class="kw">rep</span>(<span class="dv">80</span>, <span class="dv">4</span>), <span class="kw">rep</span>(<span class="dv">75</span>, <span class="dv">6</span>))))</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
## [1,]   85   80   75   75   75   75 -Inf -Inf -Inf  -Inf  -Inf  -Inf
## [2,]  Inf  Inf   80   80   80   80   75   75   75    75    75    75</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1">(ini &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">mean</span>(x), <span class="kw">sd</span>(x)))</a></code></pre></div>
<pre><code>## [1] 78.557143  5.063219</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1">(ests &lt;-<span class="st"> </span><span class="kw">optim</span>(ini, nllnormI, <span class="dt">y=</span>x, <span class="dt">YI=</span>xI)<span class="op">$</span>par)</a></code></pre></div>
<pre><code>## [1] 76.67196  5.71692</code></pre>
<p>Quando possível, é mais conveniente fazer o gráfico das superfícies de verossimilhança
na escala da 
que requer o valor da verossmimilhança avaliado nas estimativas dos parâmetros.
Vamos utilizar a função  genérica
definida em <a href="#lem:dev-generica2">1.3</a>
que pode ser usada com outras densidades com dois parâmetros.
Por conveniência definimos também a função em forma vetorizada
que utilizaremos com o comando 
na obtenção das superfícies.</p>

<div class="lemma">
<span id="lem:dev-generica2" class="lemma"><strong>Código 1.3  </strong></span><strong>Função deviance genérica.</strong>
</div>

<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1">devFun &lt;-<span class="st"> </span><span class="cf">function</span>(theta, est, llFUN, ...){</a>
<a class="sourceLine" id="cb11-2" data-line-number="2">  <span class="kw">return</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">llFUN</span>(theta, ...) <span class="op">-</span><span class="st"> </span><span class="kw">llFUN</span>(est, ...)))</a>
<a class="sourceLine" id="cb11-3" data-line-number="3">}</a>
<a class="sourceLine" id="cb11-4" data-line-number="4">devSurf &lt;-<span class="st"> </span><span class="kw">Vectorize</span>(<span class="cf">function</span>(x,y, ...) <span class="kw">devFun</span>(<span class="kw">c</span>(x,y), ...))</a></code></pre></div>
<p>O gráfico à esquerda da Figura <a href="#fig:devNormalCens2"><strong>??</strong></a> mostra superfícies
de verossimilhança na escala da 
e é obtido com os comandos a seguir.
As linhas tracejadas indicam o local do corte na superfície de deviance para obter
a verossimilhança perfilhada.
O gráfico da direita usa a parametrização <span class="math inline">\(\log(\sigma)\)</span>.
O aspecto talvez mais importante é notar que,
diferentemente dos gráficos <a href="#fig:devNormal">1.1</a>,
com dados intervalares a superfície
não mais exibe ortogonalidade entre os parâmetros.</p>
<div class="figure"><span id="fig:devNormalCens21"></span>
<img src="Exemplos2_files/figure-html/devNormalCens21-1.png" alt="Deviances de $(\mu,\sigma)$ e $(\mu,\log(\sigma))$, dados intervalares - Distribuição Normal." width="768" />
<p class="caption">
Figura 1.3: Deviances de <span class="math inline">\((\mu,\sigma)\)</span> e <span class="math inline">\((\mu,\log(\sigma))\)</span>, dados intervalares - Distribuição Normal.
</p>
</div>
<p>No código a seguir redefinimos a função de verossimilhança anterior
acrescentando alguns elementos.
Colocamos uma opção para parametrização usando <span class="math inline">\(\log(\sigma)\)</span>
através do argumento .
Comandos para verificar se argumentos de dados foram informados
permitem rodar a função mesmo sem dados pontuais
ou intervalares.
Finalmente verificamos internamente se a matriz de
dados intervalares está especificada corretamente.</p>

<div class="lemma">
<span id="lem:normalCens22" class="lemma"><strong>Código 1.4  </strong></span><strong>Redefinição da função para log-verossimilhança para dados pontuais e intervalares
de distribuição gaussiana.</strong>
</div>

<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" data-line-number="1">nllnormI &lt;-<span class="st"> </span><span class="cf">function</span>(par, yp, YI, <span class="dt">logsigma=</span><span class="ot">FALSE</span>){</a>
<a class="sourceLine" id="cb12-2" data-line-number="2">    <span class="cf">if</span>(logsigma) par[<span class="dv">2</span>] &lt;-<span class="st"> </span><span class="kw">exp</span>(par[<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb12-3" data-line-number="3">    ll1 &lt;-<span class="st"> </span><span class="kw">ifelse</span>(<span class="kw">missing</span>(yp), <span class="dv">0</span>, </a>
<a class="sourceLine" id="cb12-4" data-line-number="4">             <span class="kw">sum</span>(<span class="kw">dnorm</span>(yp, <span class="dt">mean=</span>par[<span class="dv">1</span>], <span class="dt">sd=</span>par[<span class="dv">2</span>], <span class="dt">log=</span>T)))</a>
<a class="sourceLine" id="cb12-5" data-line-number="5">    <span class="cf">if</span>(<span class="kw">missing</span>(YI)) ll2 &lt;-<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb12-6" data-line-number="6">    <span class="cf">else</span>{ </a>
<a class="sourceLine" id="cb12-7" data-line-number="7">      <span class="cf">if</span>(<span class="kw">ncol</span>(YI) <span class="op">!=</span><span class="st"> </span><span class="dv">2</span> <span class="op">||</span><span class="st"> </span><span class="kw">any</span>(YI[,<span class="dv">2</span>] <span class="op">&lt;=</span><span class="st"> </span>YI[,<span class="dv">1</span>]))</a>
<a class="sourceLine" id="cb12-8" data-line-number="8">       <span class="kw">stop</span>(<span class="st">&quot;YI deve ser matrix com 2 colunas com YI[,2] &gt; YI[,2]&quot;</span>)</a>
<a class="sourceLine" id="cb12-9" data-line-number="9">      L2 &lt;-<span class="st"> </span><span class="kw">pnorm</span>(YI, <span class="dt">mean=</span>par[<span class="dv">1</span>], <span class="dt">sd=</span>par[<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb12-10" data-line-number="10">      ll2 &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">log</span>(L2[,<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>L2[,<span class="dv">1</span>]))</a>
<a class="sourceLine" id="cb12-11" data-line-number="11">  }</a>
<a class="sourceLine" id="cb12-12" data-line-number="12">  <span class="kw">return</span>(<span class="op">-</span>(ll1 <span class="op">+</span><span class="st"> </span>ll2))</a>
<a class="sourceLine" id="cb12-13" data-line-number="13">}</a></code></pre></div>
<p>Neste exemplo fizemos a suposição de distribuição gaussiana para os dados, mas
os mesmos princípios e procedimentos são aplicáveis a outras distribuições.
O procedimento pode ser usado com dados puramente intervalares como por exemplo dados
agrupados. Suponha que os dados sejam provenientes de alguma fonte da qual<br />
se tenha apenas a distribuição (tabela) de frequências.
Podemos então definir a verossimilhança como no exemplo e obter as estimativas
mesmo sem ter acesso aos dados originais, ainda que com menor precisão.</p>
<p>Dados intervalares são muitas vezes tratados pelo termo ,
refletindo o fato de que o dado real não é observado devido a alguma restrição
(censura). Tais dados podem ocorrer, por exemplo, devido a limites de detecção de aparelhos
que podem ser incapazes de obter medidas acima (censura à direita) e/ou abaixo (censura à esquerda) de certos limites.
Outra situação são medições que por alguma razão só podem ser feitas entre dois valores (censura intervalar).
Dados censurados são discutidos em diversas áreas e entre elas são
discutidos detalhadamente no escopo de análise de sobrevivência.
Recomendamos os textos de
 e  para leitores interessados no assunto.</p>
</div>
<div id="informação-de-cada-dado" class="section level3">
<h3><span class="header-section-number">1.1.2</span> Informação de cada dado</h3>
<p>Na sessão anterior mostramos como dados pontuais e intervalares
podem ser combinados na verossimilhança.
Entretanto, a informação contida em cada um deles não é a mesma.
É intuitivo que um dado pontual contém mais informação do que um intervalar,
em especial em distribuições com um parâmetro de dispersão como a normal.
Este fato pode ser visto e descrito na verossimilhança que pode ser feita
para cada observação individualmente.
A mais informativa vai ser mais “fechada”, ou seja exibir uma maior curvatura.</p>
Para ilustrar vamos considerar o seguinte exemplo, adaptado de .
Considere <span class="math inline">\(Y \sim N(\theta, 1)\)</span> e as as seguintes observações:

<p>Sejam <span class="math inline">\(\phi(\cdot)\)</span> e <span class="math inline">\(\Phi(\cdot)\)</span> a densidade e densidade acumulada da normal padrão, respectivamente.
A verossimilhança para cada uma das observações é calculada da seguinte forma:
\begin{flalign<em>}
L(; y) &amp;= (y-)  {- (y-)^2} ; \
L_1 &amp;= L(; y=2.45) = (y-) =  {- (2.45-)^2} ; \
L_2 &amp;= L(; 0,9 &lt; y &lt; 4) = (4-) - (0,9-) ;\
L_3 &amp;= L(; y_{(5)} = 3.5) = 5{(y_{(n)} - )}^{5-1} (y_{(5)} - ).
\end{flalign</em>}
Note que a última verossimilhança decorre de um argumento multinomial e com
<span class="math display">\[F(y) = P(Y_{\{n\}} \leq y) = P[Y_{\{i\}} &lt;  y \;\forall i \neq n \mbox{ e } Y_{\{n\}} = y] \]</span></p>
<p>Os códigos para obtenção das verossimilhanças são mostrados a seguir.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1">theta.vals &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">5.5</span>, <span class="dt">l=</span><span class="dv">201</span>)</a>
<a class="sourceLine" id="cb13-2" data-line-number="2">L1 &lt;-<span class="st"> </span><span class="cf">function</span>(theta) <span class="kw">dnorm</span>(<span class="fl">2.45</span>, <span class="dt">m=</span>theta, <span class="dt">sd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb13-3" data-line-number="3">L1.vals &lt;-<span class="st"> </span><span class="kw">L1</span>(theta.vals)</a>
<a class="sourceLine" id="cb13-4" data-line-number="4"><span class="kw">plot</span>(theta.vals, L1.vals<span class="op">/</span><span class="kw">max</span>(L1.vals), <span class="dt">ty=</span><span class="st">&quot;l&quot;</span>, <span class="dt">col=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>, </a>
<a class="sourceLine" id="cb13-5" data-line-number="5">     <span class="dt">xlab=</span><span class="kw">expression</span>(theta), <span class="dt">ylab=</span><span class="kw">expression</span>(<span class="kw">LR</span>(theta)))</a>
<a class="sourceLine" id="cb13-6" data-line-number="6"><span class="co">##</span></a>
<a class="sourceLine" id="cb13-7" data-line-number="7">L2 &lt;-<span class="st"> </span><span class="cf">function</span>(theta) </a>
<a class="sourceLine" id="cb13-8" data-line-number="8">        <span class="kw">pnorm</span>(<span class="dv">4</span>,<span class="dt">mean=</span>theta,<span class="dt">sd=</span><span class="dv">1</span>)<span class="op">-</span><span class="kw">pnorm</span>(<span class="fl">0.9</span>,<span class="dt">mean=</span>theta,<span class="dt">sd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb13-9" data-line-number="9">L2.vals &lt;-<span class="st"> </span><span class="kw">L2</span>(theta.vals)</a>
<a class="sourceLine" id="cb13-10" data-line-number="10"><span class="kw">lines</span>(theta.vals, L2.vals<span class="op">/</span><span class="kw">max</span>(L2.vals), <span class="dt">ty=</span><span class="st">&quot;l&quot;</span>,  <span class="dt">lty=</span><span class="dv">5</span>, </a>
<a class="sourceLine" id="cb13-11" data-line-number="11">      <span class="dt">col=</span><span class="st">&quot;darkolivegreen&quot;</span>)</a>
<a class="sourceLine" id="cb13-12" data-line-number="12"><span class="co">##</span></a>
<a class="sourceLine" id="cb13-13" data-line-number="13">L3 &lt;-<span class="st"> </span><span class="cf">function</span>(theta) </a>
<a class="sourceLine" id="cb13-14" data-line-number="14">        <span class="dv">5</span><span class="op">*</span><span class="kw">pnorm</span>(<span class="fl">3.5</span>,<span class="dt">m=</span>theta,<span class="dt">s=</span><span class="dv">1</span>)<span class="op">^</span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="fl">3.5</span>,<span class="dt">m=</span>theta,<span class="dt">s=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb13-15" data-line-number="15">L3.vals &lt;-<span class="st"> </span><span class="kw">L3</span>(theta.vals)</a>
<a class="sourceLine" id="cb13-16" data-line-number="16"><span class="kw">lines</span>(theta.vals, L3.vals<span class="op">/</span><span class="kw">max</span>(L3.vals), <span class="dt">ty=</span><span class="st">&quot;l&quot;</span>, <span class="dt">lty=</span><span class="dv">4</span>, <span class="dt">col=</span><span class="dv">4</span>)</a></code></pre></div>
<p>Pode-se ainda considerar a função de verossimilhança conjunta das três observações que, assumindo independência,
é dada pelo produto da verossimilhanças individuais <span class="math inline">\(L(\theta) = L1 \cdot L2 \cdot L3\)</span>.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1">L4 &lt;-<span class="st"> </span><span class="cf">function</span>(theta)</a>
<a class="sourceLine" id="cb14-2" data-line-number="2">  <span class="kw">L1</span>(theta) <span class="op">*</span><span class="st"> </span><span class="kw">L2</span>(theta) <span class="op">*</span><span class="st"> </span><span class="kw">L3</span>(theta)</a>
<a class="sourceLine" id="cb14-3" data-line-number="3">L4.vals &lt;-<span class="st"> </span><span class="kw">L4</span>(theta.vals)</a>
<a class="sourceLine" id="cb14-4" data-line-number="4"><span class="kw">lines</span>(theta.vals, L4.vals<span class="op">/</span><span class="kw">max</span>(L4.vals), <span class="dt">ty=</span><span class="st">&quot;l&quot;</span>)</a>
<a class="sourceLine" id="cb14-5" data-line-number="5"><span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;y=2.45&quot;</span>, <span class="st">&quot;0,9&lt;y&lt;4&quot;</span>, <span class="st">&quot;y[5]=3.5&quot;</span>, <span class="st">&quot;conjunta&quot;</span>), </a>
<a class="sourceLine" id="cb14-6" data-line-number="6">       <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">1</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;darkolivegreen&quot;</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;black&quot;</span>))</a></code></pre></div>
<p>Curvas da função de log-verossimilhança <span class="math inline">\(l(\theta) = \log[L(\theta)]\)</span> podem ser obtidas notando que,
em geral, este é um cálculo computacionalmente mais adequado e estável.
Outra alternativa é traçar curvas da função deviance <span class="math inline">\(D(\theta) = -2[l(\theta) - l(\hat{\theta})]\)</span>.
Nos gráficos a seguir utilizamos um valor máximo computado para a sequência de valores para o parâmetro
como uma aproximação de <span class="math inline">\(l(\hat{\theta})\)</span>.
As funções de verossimilhança, log-verossimilhança e deviance (escalonadas) são
mostradas na Figura~.
Notamos no gráfico as diferentes curvaturas para
cada tipo de dado. O intervalar é o menos informativo,
seguido pelo pontual.
O máximo é mais informativo pois, além de ser pontual,
temos também a informação de sua ordenação.</p>
<div class="figure"><span id="fig:ex3"></span>
<img src="Exemplos2_files/figure-html/ex3-1.png" alt="Verossimilhanças relativas, log-verossimilhanças e deviances para as observações individuais e a conjunta." width="864" />
<p class="caption">
Figura 1.4: Verossimilhanças relativas, log-verossimilhanças e deviances para as observações individuais e a conjunta.
</p>
</div>
<p>A estimativa do parâmetro pode ser obtida de forma usual maximizando a função
de (log)verossimilhança conjunta das observações.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1">ll.ex3 &lt;-<span class="st"> </span><span class="cf">function</span>(theta){</a>
<a class="sourceLine" id="cb15-2" data-line-number="2">  l1 &lt;-<span class="st"> </span><span class="fl">-0.5</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">log</span>(<span class="dv">2</span><span class="op">*</span>pi) <span class="op">+</span><span class="st"> </span>(<span class="fl">2.45</span> <span class="op">-</span><span class="st"> </span>theta)<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb15-3" data-line-number="3">  l2 &lt;-<span class="st"> </span><span class="kw">log</span>(<span class="kw">pnorm</span>(<span class="dv">4</span>, <span class="dt">mean=</span>theta, <span class="dt">sd=</span><span class="dv">1</span>) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="fl">0.9</span>, <span class="dt">mean=</span>theta, <span class="dt">sd=</span><span class="dv">1</span>))</a>
<a class="sourceLine" id="cb15-4" data-line-number="4">  l3 &lt;-<span class="st"> </span><span class="kw">log</span>(<span class="dv">5</span>) <span class="op">+</span><span class="st"> </span><span class="dv">4</span><span class="op">*</span><span class="kw">pnorm</span>(<span class="fl">3.5</span>, <span class="dt">mean=</span>theta, <span class="dt">sd=</span><span class="dv">1</span>, <span class="dt">log=</span>T) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb15-5" data-line-number="5"><span class="st">    </span><span class="kw">dnorm</span>(<span class="fl">3.5</span>, <span class="dt">mean=</span>theta, <span class="dt">sd=</span><span class="dv">1</span>, <span class="dt">log=</span>T)</a>
<a class="sourceLine" id="cb15-6" data-line-number="6">  <span class="kw">return</span>(l1<span class="op">+</span>l2<span class="op">+</span>l3)</a>
<a class="sourceLine" id="cb15-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb15-8" data-line-number="8"><span class="kw">optimize</span>(ll.ex3, <span class="dt">interval=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">5</span>), <span class="dt">maximum=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## $maximum
## [1] 2.442739
## 
## $objective
## [1] -1.544042</code></pre>
</div>
</div>
<div id="distribuição-gama" class="section level2">
<h2><span class="header-section-number">1.2</span> Distribuição Gama</h2>

<div class="example">
<span id="exm:ex6" class="example"><strong>Example 1.3  </strong></span><strong>Exemplo - Distribuição Gama</strong>
</div>

<p>Sejam <span class="math inline">\(Y_1, Y_2, \ldots, Y_n\)</span> variáveis aleatórias independentes com distribuição Gama de parâmetros <span class="math inline">\(a\)</span> e <span class="math inline">\(s\)</span>. Nosso objetivo partindo de uma amostra aleatória <span class="math inline">\(y_1, y_2, \ldots y_n\)</span> é fazer inferências sobre os seus dois parâmetros
com seus respectivos intervalos de confiança baseados na aproximação quadrática e na função .
A função de densidade da distribuição Gama pode ser escrita na seguinte forma:
<span class="math display">\[
  f(y) = \frac{1}{s^a \Gamma(a)} y^{a-1} \exp\{-y/s\}, \quad \text{para} \quad y \ge 0 \quad \text{e} \quad a, s \ge 0.
\]</span></p>
<p>Nesta parametrização <span class="math inline">\(E(Y) = a \cdot s\)</span> e <span class="math inline">\(V(Y) = a \cdot s^2\)</span>. A função de verossimilhança é</p>
<p><span class="math display">\[\begin{align*}
L(a,s) = \prod_{i=1}^n (s^a \Gamma(a))^{-1} y_i^{a-1} \exp\{-y_i/s\} \\
        = s^{-na} \Gamma^{-n}(a) \exp\{-\sum_{i=1}^n y_i/s\} \prod_{i=1}^n y_i^{a-1}.
\end{align*}\]</span></p>
<p>Esta parametrização da função gama é comumente encontrada,
entretanto não é a mais conveniente para cálculos numéricos
pois os parâmetros não são ortogonais.
Vamos explorar estes fatos seguindo inicialmente com esta parametrização
para ilustrar o comportamento da verossimilhança.
Ao final passamos a uma forma reparametrizada
mais conveniente para implementação de algoritmos.</p>
<p>A função de log-verossimilhança é dada por
<span class="math display">\[
  l(a,s) = -n a \log s - n \log \Gamma(a) - \frac{1}{s}\sum_{i=1}^n y_i + (a-1) \sum_{i=1}^n \log y_i.
\]</span>
As funções  são obtidas derivando a log-verossimilhança em função
de cada um dos respectivos parâmetros,
<span class="math display">\[\begin{align*}
  U(a) = -n \log (s) - n \frac{\Gamma&#39;(a)}{\Gamma(a)} + \sum_{i=1}^n \log y_i \\
  U(s) = - \frac{na}{s} + \frac{1}{s^2} \sum_{i=1}^n y_i .
\end{align*}\]</span></p>
<p>Para obter as estimativas de máxima verossimilhança igualamos essas expressões a zero
e resolvemos em relação <span class="math inline">\(a\)</span> e <span class="math inline">\(s\)</span> o sistema de equações:</p>
<p><span class="math display">\[
\left\{\begin{array}{ll}
  \log(s) + \Psi(a)  &amp;= \frac{1}{n} \sum_{i=1}^n \log y_i \\
  a \cdot s &amp;= \overline{y}
\end{array} \right.
\]</span></p>
<p>em que <span class="math inline">\(\Psi(\cdot)\)</span> é a função digama ( no )
definida por <span class="math inline">\(\Psi(x) = \frac{d}{dx} \log \Gamma(x) = \Gamma&#39;(x)/\Gamma(x)\)</span>.
Este sistema claramente não tem solução analítica em <span class="math inline">\(a\)</span>, porém para <span class="math inline">\(s\)</span> obtemos
<span class="math display" id="eq:escore-gama-conc" id="eq:vero-gama-conc" id="eq:s-gama">\[\begin{equation}
\tag{1.1}
\hat{s} = \frac{\overline{y}}{a}
\end{equation}\]</span></p>
<p>Substituindo <span class="math inline">\(\hat{s}\)</span> na função de log-verossimilhança, obtemos o que chamamos de log-verossimilhança concentrada em <span class="math inline">\(a\)</span> com a expressão e escore dados por:
\begin{flalign}
\tag{1.2}
l_s(a) &amp;= -n a  - n (a) -  <em>{i=1}^n y_i + (a-1) </em>{i=1}^n y_i \
\tag{1.3}
U_s(a) &amp;= -n (/a) - n  + _{i=1}^n y_i
\end{flalign}</p>
<p>que são funções apenas do parâmetro <span class="math inline">\(a\)</span>.
Com a verossimilhança concentrada reduzimos o problema original de maximização em duas dimensões,
a uma maximação para apenas uma dimensão,
o que é mais eficiente e estável computacionalmente.
Para encontrar a estimativa de <span class="math inline">\(a\)</span> ainda precisamos maximizar a log-verossimilhança concentrada
numericamente.
Para isto temos diferentes alternativas.
Podemos usar um otimizador numérico como o implementado na função  (para um parâmetro) ou alguns dos métodos da função  (para dois ou mais parâmetros) para maximizar <a href="#eq:vero-gama-conc">(1.2)</a>.
Alternativamente, podemos obter a estimativa igualando a equacão <a href="#eq:escore-gama-conc">(1.3)</a> a zero,
e resolvendo numericamente, por exemplo com a função  do .
O pacote  implementa algoritmos adicionais incluindo
a definição e solução de sistemas de equações.</p>
Em geral, os métodos numéricos requerem valores iniciais para os parâmetros para inicializar o algoritmo.
No caso da Gamma há uma escolha possível dos valores iniciais que seriam os estimadores pelo
método dos momentos.
Para a parametrização da Gama adotada aqui temos que as médias e variâncias populacionais e amostrais são:

<p>Igualando os amostrais aos respectivos populacionais temos que estes estimadores são dados por:
<span class="math display">\[
\hat{a}_M = \frac{\bar{Y}}{\hat{s}_M} \;\; \mbox{ e }\;\; \hat{s}_M = \frac{\hat{\sigma}^2}{\bar{Y}} .
\]</span>
Uma aproximação seria substituir <span class="math inline">\(\hat{sigma}^2\)</span> pela variância amostral.
O uso de bons valores iniciais garante um melhor comportamento dos algoritmos numéricos.</p>
<p>Mas antes disso, vamos investigar a ortogonalidade entre <span class="math inline">\(a\)</span> e <span class="math inline">\(s\)</span>.
Para isto, precisamos obter a matriz de segundas derivadas,
neste caso de dimensão <span class="math inline">\(2 \times 2\)</span>, que fornece
a matriz de informação observada e/ou esperada.</p>
<p>Derivando as funções escore temos,
\begin{flalign*}
 = -n \
 =  -  _{i=1}^n y_i \
 = -.
\end{flalign*}
Logo, a matriz de informação observada é dada por,
<span class="math display">\[
  I_o(a,s) = \begin{bmatrix}
n\left [ \frac{\Gamma&#39;&#39;(a)}{\Gamma&#39;(a)} - \left ( \frac{\Gamma&#39;(a)}{\Gamma(a)} \right )^2 \right ] &amp; \frac{n}{s} \\ 
\frac{n}{s} &amp; - \frac{na}{s^2} + \frac{2}{s^3 \sum_{i=1}^n y_i} 
\end{bmatrix} .
\]</span>
A matriz esperada é obtida tomando a esperança da matriz observada, lembrando que <span class="math inline">\(E(Y) = a \cdot s\)</span>, temos
<span class="math display">\[
  I_E(a,s) = \begin{bmatrix}
n\left [ \frac{\Gamma&#39;&#39;(a)}{\Gamma(a)} - \left ( \frac{\Gamma&#39;(a)}{\Gamma(a)} \right )^2 \right ] &amp; \frac{n}{s} \\ 
\frac{n}{s} &amp; \frac{na}{s^2}.
\end{bmatrix} .
\]</span></p>
<p>Os termos fora da diagonal são não-nulos o que mostra que os parâmetros são não ortogonais.
Para visualizarmos o formato da função de log-verossimilhança a Figura <a href="#fig:devgama">1.5</a> apresenta a superfície de log-verossimilhança e sua aproximação quadrática em escala de 
para facilitar a construção e visualização do gráfico.
Os dados utilizados foram gerados da distribuiição Gama, com parâmetros <span class="math inline">\(a=10\)</span> e <span class="math inline">\(s = 5\)</span>.</p>
<div class="figure"><span id="fig:devgama"></span>
<img src="Exemplos2_files/figure-html/devgama-1.png" alt="Deviance exata e aproximada por tamanho de amostra - Distribuição Gama." width="384" />
<p class="caption">
Figura 1.5: Deviance exata e aproximada por tamanho de amostra - Distribuição Gama.
</p>
</div>
<p>Pelos gráficos podemos ver claramente que quando o tamanho da amostra é pequeno <span class="math inline">\(n=10\)</span> o formato da log-verossimilhança é extremamente assimétrico e consequentemente a aproximação quadrática é muito ruim. Com o aumento da amostra a aproximação quadrática vai melhorando, até que com <span class="math inline">\(n=2000\)</span> a diferença é bastante pequena. Os gráficos também mostram a dependência entre os parâmetros <span class="math inline">\(a\)</span> e <span class="math inline">\(s\)</span>, quando o <span class="math inline">\(a\)</span> aumenta necessariamente o <span class="math inline">\(s\)</span> diminui para manter a média que é <span class="math inline">\(a\cdot s\)</span>, além disso fica claro também que a incerteza associada a estimativa de <span class="math inline">\(a\)</span> é muito maior quando comparada a estimativa de <span class="math inline">\(s\)</span>.</p>
<p>Agora retornamos à obtenção das estimativas de máxima verossimilhança.
Lembrando que a log-verossimilhança concentrada <a href="#eq:vero-gama-conc">(1.2)</a>
é uma função apenas do parâmetro <span class="math inline">\(a\)</span>, ama vez obtido a estimativa <span class="math inline">\(\hat{a}\)</span> podemos substitui-la em <span class="math inline">\(\hat{s} = \frac{\overline{y}}{\hat{a}}\)</span> e obter a estimativa de <span class="math inline">\(s\)</span>. Da mesma forma podemos substituir as estimativas nas matrizes de informação observada e esperada e encontrar intervalos de confiança assintóticos, sabendo que estes intervalos serão consistentes apenas com amostras grandes. Mas para todos estes procedimentos precisamos maximizar a log-verossimilhança concentrada em <span class="math inline">\(a\)</span>. A forma mais comum de fazer isso é usando o algoritmo de Newton-Raphson que utiliza a informação observada,
ou uma variante deste chamada de algoritmo Escore de Fisher
que substitui a informação observada pela esperada.
Vamos abrir um parenteses e explicar rapidamente o algoritmo de Newton-Raphson.</p>
O método de Newton-Raphson é usado para se obter a solução numérica de uma equação na forma <span class="math inline">\(f(x) = 0\)</span>, onde <span class="math inline">\(f(x)\)</span> é contínua e diferenciável e sua equação possui uma solução próxima a um ponto dado. O processo de solução começa com a escolha do ponto <span class="math inline">\(x_1\)</span> como a primeira tentativa de solução. A segunda tentativa, <span class="math inline">\(x_2\)</span>, é obtida a partir do cruzamento com o eixo <span class="math inline">\(x\)</span> da reta tangente a <span class="math inline">\(f(x)\)</span> no ponto <span class="math inline">\((x_1, f(x_1))\)</span>. A tentativa seguinte, <span class="math inline">\(x_3\)</span> é a intersecção com o eixo <span class="math inline">\(x\)</span> da reta tangente a <span class="math inline">\(f(x)\)</span> no ponto <span class="math inline">\((x_2,f(x_2))\)</span>, e assim por diante. A equação de iteração é dada por:
<span class="math display">\[\begin{equation}
  x_{i+1} = x_i - \frac{f(x_i)}{f&#39;(x_i)}
\end{equation}\]</span>
ela é chamada de equação de iteração porque a solução é obtida com a aplicação repetida em cada valor sucessivo de <span class="math inline">\(i\)</span> até que um critério de convergência seja atingido. Diversos critérios de convergência podem ser usados. Os mais comuns são:

<p>Uma função chamada  é
definida no código <a href="#lem:NR1">1.5</a>.</p>

<div class="lemma">
<span id="lem:NR1" class="lemma"><strong>Código 1.5  </strong></span><strong>Algoritmo genérico de Newton-Raphson.</strong>
</div>

<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1">NewtonRaphson &lt;-<span class="st"> </span><span class="cf">function</span>(initial, escore, hessiano, <span class="dt">tol=</span><span class="fl">0.0001</span>, </a>
<a class="sourceLine" id="cb17-2" data-line-number="2">                   max.iter, n.dim, <span class="dt">print=</span><span class="ot">FALSE</span>, ...){</a>
<a class="sourceLine" id="cb17-3" data-line-number="3">  solucao &lt;-<span class="st"> </span>initial</a>
<a class="sourceLine" id="cb17-4" data-line-number="4">  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>max.iter){</a>
<a class="sourceLine" id="cb17-5" data-line-number="5">    solucao &lt;-<span class="st"> </span>initial <span class="op">-</span><span class="st"> </span><span class="kw">solve</span>(<span class="kw">hessiano</span>(initial, ...), </a>
<a class="sourceLine" id="cb17-6" data-line-number="6">                <span class="kw">escore</span>(initial, ...))</a>
<a class="sourceLine" id="cb17-7" data-line-number="7">    tolera &lt;-<span class="st"> </span><span class="kw">abs</span>(solucao <span class="op">-</span><span class="st"> </span>initial)</a>
<a class="sourceLine" id="cb17-8" data-line-number="8">    <span class="cf">if</span>(<span class="kw">all</span>(tolera <span class="op">&lt;</span><span class="st"> </span>tol) <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>)<span class="cf">break</span></a>
<a class="sourceLine" id="cb17-9" data-line-number="9">    initial &lt;-<span class="st"> </span>solucao</a>
<a class="sourceLine" id="cb17-10" data-line-number="10">    <span class="cf">if</span>(print) <span class="kw">print</span>(initial)</a>
<a class="sourceLine" id="cb17-11" data-line-number="11">  }</a>
<a class="sourceLine" id="cb17-12" data-line-number="12">  <span class="kw">return</span>(initial)</a>
<a class="sourceLine" id="cb17-13" data-line-number="13">} </a></code></pre></div>
<p>Note que para usar este algoritmo é necessário obter a primeira (escore) e a segunda (hessiano) derivada.
Neste exemplo é possível obter expressões analíticas para ambas.
Em modelos mais complexos expressões analíticas podem ser substituídas por
gradientes e hessianos obtidos por algoritmos numéricos.
Além disto, em certos casos o custo computacional em calcular o hessiano analítico pode ser muito maior que o numérico, o que acontece em alguns modelos multivariados que em geral envolvem muitas inversões de matrizes densas, fazendo com que este algoritmo se torne muito lento.</p>
<p>Cabe ressaltar que o método de Newton-Raphson é um algoritmo para encontrar raízes de uma equação que no caso da função escore leva as estimativas de máxima verossimilhança.
Porém, existem diversos e poderosos algoritmos de maximização numérica que não
exigem derivadas analíticas embora possam ser beneficiados com o uso de resultados destas principalmente a função escore. No  alguns destes maximizadores numéricos estão implementados na função .</p>
<p>Continuando com o exemplo da Gama, vamos obter a função escore e o hessiano da função de log-verossimilhança concentrada e usar o algoritmo de Newton-Raphson para obter a estimativa de <span class="math inline">\(a\)</span>.<br />
A partir de <a href="#eq:escore-gama-conc">(1.3)</a> temos que:
\begin{flalign*}
U_s(a) &amp;= -n (/a) - n (a) + _{i=1}^n y_i \
U_s’(a) &amp;=  - n ’(a)
\end{flalign*}
em que <span class="math inline">\(\Psi&#39;(a)\)</span> é a função trigama que é a derivada segunda do logaritmo da função gama.</p>
<p>Escrevendo estas funções em  temos o código <a href="#lem:escore-hess-gama">1.6</a>.</p>

<div class="lemma">
<span id="lem:escore-hess-gama" class="lemma"><strong>Código 1.6  </strong></span><strong>Função escore e hessiana ambas em relação ao parâmetro <span class="math inline">\(a\)</span> da distribuição Gama.</strong>
</div>

<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" data-line-number="1">escore &lt;-<span class="st"> </span><span class="cf">function</span>(a,y){</a>
<a class="sourceLine" id="cb18-2" data-line-number="2">  n &lt;-<span class="st"> </span><span class="kw">length</span>(y)</a>
<a class="sourceLine" id="cb18-3" data-line-number="3">  u &lt;-<span class="st"> </span><span class="op">-</span>n<span class="op">*</span><span class="kw">log</span>(<span class="kw">mean</span>(y)<span class="op">/</span>a) <span class="op">-</span><span class="st"> </span>n<span class="op">*</span><span class="kw">digamma</span>(a) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">log</span>(y))</a>
<a class="sourceLine" id="cb18-4" data-line-number="4">  <span class="kw">return</span>(u)}</a>
<a class="sourceLine" id="cb18-5" data-line-number="5"></a>
<a class="sourceLine" id="cb18-6" data-line-number="6">hessiano &lt;-<span class="st"> </span><span class="cf">function</span>(a,y){</a>
<a class="sourceLine" id="cb18-7" data-line-number="7">  n &lt;-<span class="st"> </span><span class="kw">length</span>(y)</a>
<a class="sourceLine" id="cb18-8" data-line-number="8">  u.l &lt;-<span class="st"> </span>(n<span class="op">/</span>a)<span class="op">-</span><span class="kw">trigamma</span>(a)<span class="op">*</span>n</a>
<a class="sourceLine" id="cb18-9" data-line-number="9">  <span class="kw">return</span>(u.l)}</a></code></pre></div>
<p>Gerando 100 valores da distribuição Gama com parametros <span class="math inline">\(a = 10\)</span> e <span class="math inline">\(s = 5\)</span>, obtemos os valores iniciais aproximados ou exatos correspondendo aos estimadores dos momentos:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb19-2" data-line-number="2">y100 &lt;-<span class="st"> </span><span class="kw">rgamma</span>(<span class="dv">100</span>,<span class="dt">shape=</span><span class="dv">10</span>,<span class="dt">scale=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb19-3" data-line-number="3">My &lt;-<span class="st"> </span><span class="kw">mean</span>(y100) ; Vy &lt;-<span class="st"> </span><span class="kw">var</span>(y100)</a>
<a class="sourceLine" id="cb19-4" data-line-number="4">(initAprox &lt;-<span class="st"> </span><span class="kw">c</span>(My<span class="op">^</span><span class="dv">2</span><span class="op">/</span>Vy , Vy<span class="op">/</span>My))</a></code></pre></div>
<pre><code>## [1] 13.296869  3.679325</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb21-1" data-line-number="1">n &lt;-<span class="st"> </span><span class="kw">length</span>(y100) ; Vy &lt;-<span class="st"> </span>Vy <span class="op">*</span><span class="st"> </span>(n<span class="dv">-1</span>)<span class="op">/</span>n</a>
<a class="sourceLine" id="cb21-2" data-line-number="2">(init &lt;-<span class="st"> </span><span class="kw">c</span>(My<span class="op">^</span><span class="dv">2</span><span class="op">/</span>Vy , Vy<span class="op">/</span>My))</a></code></pre></div>
<pre><code>## [1] 13.431181  3.642532</code></pre>
<p>O passo final para obter a estimativa de máxima verossimilhança de <span class="math inline">\(a\)</span> é usar o algoritmo de Newton-Raphson.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" data-line-number="1">(a.hat &lt;-<span class="st"> </span><span class="kw">NewtonRaphson</span>(<span class="dt">initial =</span> init[<span class="dv">1</span>], <span class="dt">escore =</span> escore , </a>
<a class="sourceLine" id="cb23-2" data-line-number="2">       <span class="dt">hessiano=</span>hessiano, <span class="dt">max.iter=</span><span class="dv">100</span>, <span class="dt">n.dim=</span><span class="dv">1</span>, <span class="dt">y=</span>y100))</a></code></pre></div>
<pre><code>## [1] 13.53678</code></pre>
<p>Definindo o argumento  é possível visualizar todas as tentativas do algoritmo até a convergência. Neste exemplo foi necessário seis iterações para atingir o critério de convergência. Uma limitação deste algoritmo é que o chute inicial não pode estar muito longe da solução, o que pode ser difícil de obter em modelos complexos, nestes casos estimativas grosseiras por mínimos quadrados podem ser de grande valia como valores iniciais.</p>
<p>Uma vez estimado o <span class="math inline">\(a\)</span> podemos substituir na expressão de <span class="math inline">\(\hat{s}\)</span> para obtê-lo,</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb25-1" data-line-number="1">(s.hat &lt;-<span class="st"> </span><span class="kw">mean</span>(y100)<span class="op">/</span>a.hat)</a></code></pre></div>
<pre><code>## [1] 3.614117</code></pre>
<p>Para construção dos intervalos assintóticos basta substituir as estimativas nas matrizes de informação observada e/ou esperada. Note que, no caso da distribuição Gama a distribuição assintótica do vetor <span class="math inline">\((\hat{a},\hat{s})^\top\)</span> é a seguinte,</p>
<p><span class="math display">\[
  \begin{bmatrix}
\hat{a} \\ \hat{s} 
\end{bmatrix} \sim NM_2 \left ( \begin{bmatrix}
a \\ s  
\end{bmatrix}, \begin{bmatrix}
n \Psi&#39;(\hat{a}) &amp; n /\hat{s} \\ 
n / \hat{s} &amp;  n\hat{a} / \hat{s}^2 
\end{bmatrix}^{-1} \right ) 
\]</span>
poderíamos usar também a matriz de informação observada que é assintoticamente equivalente. O código <a href="#lem:IoIe-Gama">1.7</a> implementa a matriz de informação esperada e observada e constrói os intervalos de confiança assintóticos para <span class="math inline">\(a\)</span> e <span class="math inline">\(s\)</span>.</p>

<div class="lemma">
<span id="lem:IoIe-Gama" class="lemma"><strong>Código 1.7  </strong></span><strong>Funções para a matriz de informação esperada e informação observada da distribuição Gama.</strong>
</div>

<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb27-1" data-line-number="1">Ie &lt;-<span class="st"> </span><span class="cf">function</span>(a,s,y){</a>
<a class="sourceLine" id="cb27-2" data-line-number="2">  n &lt;-<span class="st"> </span><span class="kw">length</span>(y)</a>
<a class="sourceLine" id="cb27-3" data-line-number="3">  saida &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(n<span class="op">*</span><span class="kw">trigamma</span>(a),n<span class="op">/</span>s,</a>
<a class="sourceLine" id="cb27-4" data-line-number="4">          n<span class="op">/</span>s, (n<span class="op">*</span>a)<span class="op">/</span>s<span class="op">^</span><span class="dv">2</span>),<span class="dv">2</span>,<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb27-5" data-line-number="5">  <span class="kw">return</span>(saida)}</a>
<a class="sourceLine" id="cb27-6" data-line-number="6"></a>
<a class="sourceLine" id="cb27-7" data-line-number="7">Io &lt;-<span class="st"> </span><span class="cf">function</span>(a,s,y){</a>
<a class="sourceLine" id="cb27-8" data-line-number="8">  n &lt;-<span class="st"> </span><span class="kw">length</span>(y)</a>
<a class="sourceLine" id="cb27-9" data-line-number="9">  saida &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(n<span class="op">*</span><span class="kw">trigamma</span>(a), n<span class="op">/</span>s, n<span class="op">/</span>s,</a>
<a class="sourceLine" id="cb27-10" data-line-number="10">         <span class="op">-</span>(n<span class="op">*</span>a)<span class="op">/</span>s<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>(<span class="dv">2</span><span class="op">/</span>s<span class="op">^</span><span class="dv">3</span>)<span class="op">*</span><span class="kw">sum</span>(y)),<span class="dv">2</span>,<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb27-11" data-line-number="11">  <span class="kw">return</span>(saida)}</a></code></pre></div>
<p>Avaliando as matrizes no ponto de máximo,</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb28-1" data-line-number="1"><span class="kw">Ie</span>(<span class="dt">a =</span> a.hat, <span class="dt">s =</span> s.hat, <span class="dt">y=</span>y100)</a></code></pre></div>
<pre><code>##           [,1]      [,2]
## [1,]  7.666854  27.66927
## [2,] 27.669273 103.63604</code></pre>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb30-1" data-line-number="1"><span class="kw">Io</span>(<span class="dt">a =</span> a.hat, <span class="dt">s =</span> s.hat, <span class="dt">y=</span>y100)</a></code></pre></div>
<pre><code>##           [,1]      [,2]
## [1,]  7.666854  27.66927
## [2,] 27.669273 103.63604</code></pre>
<p>Como é possível observar a matriz de informação esperada e observada apesar de apresentarem formas diferentes levam a resultados idênticos para o tamanho de amostra <span class="math inline">\(n=100\)</span> considerado aqui. Sendo assim, vamos usar apenas a informação esperada que é mais simples de avaliar. Para obter os intervalos assintóticos basta inverter a matriz de informação e pegar os termos em sua diagonal que corresponde a variância de <span class="math inline">\(\hat{a}\)</span> e <span class="math inline">\(\hat{s}\)</span>.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb32-1" data-line-number="1">erro.padrao &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">solve</span>(<span class="kw">Ie</span>(<span class="dt">a =</span> a.hat, <span class="dt">s=</span> s.hat, <span class="dt">y=</span>y100))))</a>
<a class="sourceLine" id="cb32-2" data-line-number="2">(ic.a &lt;-<span class="st"> </span>a.hat <span class="op">+</span><span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span><span class="kw">qnorm</span>(<span class="fl">0.975</span>)<span class="op">*</span>erro.padrao[<span class="dv">1</span>])</a></code></pre></div>
<pre><code>## [1]  9.829956 17.243600</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" data-line-number="1">(ic.s &lt;-<span class="st"> </span>s.hat <span class="op">+</span><span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span><span class="kw">qnorm</span>(<span class="fl">0.975</span>)<span class="op">*</span>erro.padrao[<span class="dv">2</span>])</a></code></pre></div>
<pre><code>## [1] 2.605898 4.622336</code></pre>
<p>Outra forma é a construção de intervalos baseados no perfil de verossimilhança.
As funções em <a href="#lem:pllab-gamma">1.8</a> implementam
o perfil de verossimilhança para os parâmetros <span class="math inline">\(a\)</span> e <span class="math inline">\(s\)</span>, respectivamente.</p>

<div class="lemma">
<span id="lem:pllab-gamma" class="lemma"><strong>Código 1.8  </strong></span><strong>Funções para a matriz de informação esperada e informação observada da distribuição Gama.</strong>
</div>

<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb36-1" data-line-number="1">perf.a &lt;-<span class="st"> </span><span class="cf">function</span>(s, a, dados){</a>
<a class="sourceLine" id="cb36-2" data-line-number="2">   ll &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">dgamma</span>(dados, <span class="dt">shape=</span>a, <span class="dt">scale=</span>s, <span class="dt">log=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb36-3" data-line-number="3">   <span class="kw">return</span>(ll)}</a>
<a class="sourceLine" id="cb36-4" data-line-number="4">perf.s &lt;-<span class="st"> </span><span class="cf">function</span>(a, s, dados){</a>
<a class="sourceLine" id="cb36-5" data-line-number="5">   ll &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">dgamma</span>(dados, <span class="dt">shape=</span>a, <span class="dt">scale=</span>s, <span class="dt">log=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb36-6" data-line-number="6">   <span class="kw">return</span>(ll)}</a></code></pre></div>
<p>Para as maximizações necessárias vamos utilizar a função  própria para maximização em uma dimensão como é o caso aqui. Precisamos também criar uma grade para a avaliação da função. Também será avaliada a verossimilhança condicional para comparação de forma similar ao mostrado no Exemplo <a href="#sec:vero-normal"><strong>??</strong></a>.
A Figura <a href="#fig:perfgama">1.6</a> apresenta os resultados.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb37-1" data-line-number="1">grid.a &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">9</span>,<span class="dv">18</span>,<span class="dt">l=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb37-2" data-line-number="2">grid.s &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">2</span>,<span class="dv">5</span>,<span class="dt">l=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb37-3" data-line-number="3"><span class="co">## Perfil para a</span></a>
<a class="sourceLine" id="cb37-4" data-line-number="4">vero.perf.a &lt;-<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb37-5" data-line-number="5"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(grid.a)){</a>
<a class="sourceLine" id="cb37-6" data-line-number="6">vero.perf.a[i] &lt;-<span class="st"> </span><span class="kw">optimize</span>(perf.a,<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">200</span>), </a>
<a class="sourceLine" id="cb37-7" data-line-number="7">          <span class="dt">a=</span>grid.a[i],<span class="dt">dados=</span>y100,<span class="dt">maximum=</span><span class="ot">TRUE</span>)<span class="op">$</span>objective}</a>
<a class="sourceLine" id="cb37-8" data-line-number="8"></a>
<a class="sourceLine" id="cb37-9" data-line-number="9"><span class="co">## Perfil para s</span></a>
<a class="sourceLine" id="cb37-10" data-line-number="10">vero.perf.s &lt;-<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb37-11" data-line-number="11"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(grid.s)){</a>
<a class="sourceLine" id="cb37-12" data-line-number="12">vero.perf.s[i] &lt;-<span class="st"> </span><span class="kw">optimize</span>(perf.s,<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1000</span>), </a>
<a class="sourceLine" id="cb37-13" data-line-number="13">                  <span class="dt">s=</span>grid.s[i],<span class="dt">dados=</span>y100,<span class="dt">maximum=</span><span class="ot">TRUE</span>)<span class="op">$</span>objective}</a>
<a class="sourceLine" id="cb37-14" data-line-number="14"></a>
<a class="sourceLine" id="cb37-15" data-line-number="15"><span class="co">## Condicional para a</span></a>
<a class="sourceLine" id="cb37-16" data-line-number="16">vero.cond.a &lt;-<span class="st"> </span><span class="kw">sapply</span>(grid.a,perf.a,<span class="dt">s=</span>s.hat,<span class="dt">dados=</span>y100)</a>
<a class="sourceLine" id="cb37-17" data-line-number="17"></a>
<a class="sourceLine" id="cb37-18" data-line-number="18"><span class="co">## Condicional para sigma</span></a>
<a class="sourceLine" id="cb37-19" data-line-number="19">vero.cond.s &lt;-<span class="st"> </span><span class="kw">sapply</span>(grid.s, perf.s, <span class="dt">a=</span> a.hat , <span class="dt">dados=</span>y100)</a></code></pre></div>
<div class="figure"><span id="fig:perfgama"></span>
<img src="Exemplos2_files/figure-html/perfgama-1.png" alt="Deviance perfilhada, condicional e limites do intervalo de confiança assintótico para a e s - Distribuição Gama." width="960" />
<p class="caption">
Figura 1.6: Deviance perfilhada, condicional e limites do intervalo de confiança assintótico para a e s - Distribuição Gama.
</p>
</div>
<p>Como mostra a Figura <a href="#fig:perfgama">1.6</a> os intervalos obtidos condicionando a log-verossimilhança na estimativa de máxima verossimilhança são claramente mais curtos que o intervalo baseado em perfil de verossimilhança e o intervalo assintótico. Comparando o perfil com o assintótico verificamos que a perfilhada traz intervalos ligeiramente assimétricos e mais largos que a aproximação quadrática, a aproximação é ligeiramente deslocada para esquerda e para direita para os parâmetros <span class="math inline">\(a\)</span> e <span class="math inline">\(s\)</span> respectivamente.</p>
<div id="parametrizações-para-gama" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Parametrizações para Gama</h3>
<p>Vamos explorar este exemplo para ilustrar o efeito de diferentes parametrizações
no formato da verossimilhança da densidade Gama.
Aproveitamos ainda esta sessão para ilustrar o uso de
algumas sintaxes e formas de programação em .</p>

<p>Esta é a parametrização utilizada anteriormente e também a usada
nas funções  do .
O parâmetro de forma () é
<span class="math inline">\(\alpha=a\)</span> e o de escala () <span class="math inline">\(\beta=s\)</span>.
Lembrando as expressões obtidas anteriormente temos:</p>
<p>\begin{flalign<em>}
&amp; ;;; Y {}(, ) &amp;\
&amp; f(y|, ) = ;y^{-1};{-y/}
y 0 ;,; 0 ;,; &gt; 0 &amp; \
&amp; ;;;E[Y] = ;;; Var[Y] = ^2 &amp; \
&amp; L((,)|y) = ()^n; <em>{i=1}^{n} y_i^{-1};{- </em>{i=1}^{n} y_i/} &amp;\
&amp; l((,)|y) = n(-(()) - () +<br />
(-1)  - {y}/) &amp;
\end{flalign</em>}</p>
A função escore é escrita como:

Igualando as funções a zero, da primeira equação temos
<span class="math inline">\(\hat{\alpha} \hat{\beta} = \bar{y}\)</span>. Substituindo <span class="math inline">\(\beta\)</span> por <span class="math inline">\(\hat{\beta}\)</span>
a segunda expressão é escrita como:
<span class="math display">\[
n \left(-\frac{\Gamma&#39;(\alpha)}{\Gamma(\alpha)} -\log\left(\frac{\bar{y}}{\hat{\alpha}}\right) + \overline{\log y}\right) = 0 \]</span>
O EMV é portanto solução conjunta de

<p>em que
<span class="math inline">\(\psi(t) = \frac{d}{dt} \log(\Gamma(t)) = \frac{\Gamma&#39;(t)}{\Gamma(t)}\)</span>
(função no ) e <span class="math inline">\(\overline{\log y} = \sum_{i=1}^n \log(y_i)/n\)</span>.</p>

<p>Esta parametrização utilizada por
, dentre outros autores, é a parametrização
original usada na linguagem  e sua primeira implementação no programa 
.
Neste caso o parâmetro de escala é trocado pela seu inverso, a taxa ()
e denotamos <span class="math inline">\(\lambda=1/\beta\)</span>.
No  pode-se definir a escala ou taxa.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb38-1" data-line-number="1"><span class="kw">args</span>(rgamma)</a></code></pre></div>
<pre><code>## function (n, shape, rate = 1, scale = 1/rate) 
## NULL</code></pre>
<p>As expressões ficam então:
<span class="math display">\[\begin{align*}
  Y &amp;\sim {\rm G}(\alpha, \lambda)  \\
  f(y|\alpha, \lambda) &amp;= \frac{\lambda^\alpha}{\Gamma(\alpha)}\;y^{\alpha-1}\;\exp\{-\lambda y\}\;\;\;;
  y \geq 0  \;,\; \alpha \geq 0   \;,\; \lambda &gt; 0 \\
  E[Y] &amp;= \alpha/\lambda    \;\;\; Var[Y] = \alpha / \lambda^2 \\
  L((\alpha,\lambda)|y) &amp;= \left(\frac{\lambda^\alpha}{\Gamma(\alpha)}\right)^n\; \prod_{i=1}^{n} y_i^{\alpha-1}\;\exp\{- \lambda \sum_{i=1}^{n} y_i\}\\
  l((\alpha,\lambda)|y) &amp;= n\left(\alpha \log(\lambda)-\log(\Gamma(\alpha))  + 
      (\alpha-1) \overline{\log(y)} - \lambda\bar{y}\right)
\end{align*}\]</span></p>
<p>Função escore:
<span class="math display">\[\begin{align*}
\left\{ \begin{array}{ll} 
\frac{d l}{d\lambda} &amp;= n\left(\frac{\alpha}{\lambda} - \bar{y}\right) \\
\frac{d l}{d\alpha}        &amp;= n \left(\log(\lambda) - \frac{\Gamma&#39;(\alpha)}{\Gamma(\alpha)} + \overline{\log y}\right)
\end{array}  \right.
\end{align*}\]</span>
Igualando as funções a zero, da primeira equação temos <span class="math inline">\(\hat{\lambda} = \hat{\alpha}/\bar{y}\)</span>. Substituindo <span class="math inline">\(\lambda\)</span> por <span class="math inline">\(\hat{\lambda}\)</span>
a segunda expressão é escrita como:
<span class="math display">\[n \left(\log \left(\frac{\hat{\alpha}}{\bar{y}} \right)+ \overline{\log y} - \frac{\Gamma&#39;(\alpha)}{\Gamma(\alpha)}\right) = 0 \]</span>
O EMV é solução conjunta de
<span class="math display">\[\begin{align*}
\left\{\begin{array}{ll}
 \log \lambda + \overline{\log y} &amp;= \psi(\lambda \bar{y})\\
\bar{y} &amp;= \alpha/\lambda
\end{array} \right.
\end{align*}\]</span></p>

<p> menciona ainda duas parametrizações sendo a primeira considerada a mais usual,
e sendo a mesma que é implementada no . Uma segunda é parametrizada por
<span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\mu = \alpha \beta\)</span> com o segundo parâmetro correspondente à média da variável.</p>
<p>Esta segunda parametrização tem propriedades interessantes para inferência.
A primeira é a ortogonalidade na matriz de informação entre <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\mu\)</span>.
Além disto em geral <span class="math inline">\(\mu\)</span> é o
usualmente o parâmetro de interesse para inferências e <span class="math inline">\(\alpha\)</span> é um parâmetro .
A parametrização é adequada para modelagem estatística
na qual usualmente se propõe um modelo
de regressão para média <span class="math inline">\(\mu\)</span>, como por exemplo em modelos lineares generalizados (MLG).
<span class="math display">\[\begin{align*}
  Y &amp;\sim {\rm G}(\alpha, \mu)  \\
  f(y|\alpha, \mu) &amp;= \frac{\alpha^\alpha}{\Gamma(\alpha)\,\mu^\alpha}\;y^{\alpha-1}\;\exp\{- \alpha y/\mu\}\;\;\;;
  y \geq 0  \;,\; \alpha \geq 0   \;,\; \mu \geq 0 \\
  E[Y] &amp;= \mu    \;\;\; Var[Y] = \mu^2/\alpha \\
  L((\alpha,\mu)|y) &amp;= \left(\frac{\alpha^\alpha}{\Gamma(\alpha) \mu^\alpha}\right)^n                 
\prod_{i=1}^{n} y_i^{\alpha-1}\;\exp\{- \alpha \sum_{i=1}^{n}  y_i/\mu\}\\
  l((\alpha,\mu)|y) &amp;= n\left(\alpha (\log(\alpha)  - \log(\mu)) - \log(\Gamma(\alpha)) +  
            (\alpha-1) \overline{\log(y)} - \alpha\bar{y}/\mu \right)
\end{align*}\]</span>
Função escore
<span class="math display">\[\begin{align*}
\left\{ \begin{array}{ll} 
\frac{d l}{d\mu} &amp;= n\left(-\frac{\alpha}{\mu} + \frac{\alpha\bar{y}}{\mu^2}\right) \\
\frac{d l}{d\alpha}        &amp;= n \left(\log(\alpha) + 1 - \log(\mu) - \frac{\Gamma&#39;(\alpha)}{\Gamma(\alpha)} + \overline{\log y} - \frac{\bar{y}}{\mu}\right)
\end{array}  \right.
\end{align*}\]</span>
Igualando as funções a zero, da primeira equação temos <span class="math inline">\(\hat{\mu} = \bar{y}\)</span>.
Substituindo <span class="math inline">\(\mu\)</span> por <span class="math inline">\(\hat{\mu}\)</span>
a segunda expressão é escrita como:
<span class="math display">\[n \left(\log \hat{\alpha} - \log(\bar{y}) - \frac{\Gamma&#39;(\hat{\alpha})}{\Gamma(\hat{\alpha})} + \overline{\log y}\right) = 0 \]</span>
O EMV é solução conjunta de:
<span class="math display">\[\begin{align*}
\left\{\begin{array}{ll}
 \log \hat{\alpha} - \psi(\hat{\alpha}) &amp;= \log \bar{y} - \overline{\log y}  \\
\hat{\mu} &amp;= \bar{y} 
\end{array} \right.
\end{align*}\]</span>
Nesta parametrização, a partir das expressões de <span class="math inline">\(\frac{d^2 l}{d\mu^2}\)</span> e <span class="math inline">\(\frac{d^2 l}{d\alpha^2}\)</span>
obtemos que os parametros são ortogonais na informação já que
<span class="math inline">\(I_E(\mu,\alpha)\)</span> e <span class="math inline">\(I_E(\hat{\mu},\hat{\alpha})\)</span> são matrizes diagonais.</p>
<p>Para obter os gráficos de verossimilhança vamos definir uma função
em  que será escrita com opção para as três parametrizações mencionadas.
Em todas estas parametrizações os parâmetros são não negativos e
uma transformação logarítmica fornece parâmetros com suporte na reta real,
mas note que isto exclui o valor nulo do espaço paramétrico.
Desta forma nossa função permite ainda reparametrizações adicionais trocando
os parâmetros por seus logaritmos.</p>

<div class="lemma">
<span id="lem:Vero-Gama" class="lemma"><strong>Código 1.9  </strong></span><strong>Funções de verossimilhança de distribuição Gama, para diferentes parametrizações.</strong>
</div>

<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb40-1" data-line-number="1">neglLik &lt;-<span class="st"> </span><span class="cf">function</span>(par, amostra, <span class="dt">modelo=</span><span class="dv">2</span>, <span class="dt">logpar=</span><span class="ot">FALSE</span>){</a>
<a class="sourceLine" id="cb40-2" data-line-number="2">  <span class="cf">if</span>(logpar) par &lt;-<span class="st"> </span><span class="kw">exp</span>(par)</a>
<a class="sourceLine" id="cb40-3" data-line-number="3">  ll &lt;-<span class="st"> </span><span class="cf">switch</span>(modelo, </a>
<a class="sourceLine" id="cb40-4" data-line-number="4">    <span class="st">&quot;1&quot;</span> =<span class="st"> </span>{alpha &lt;-<span class="st"> </span>par[<span class="dv">1</span>]; beta &lt;-<span class="st"> </span>par[<span class="dv">2</span>];</a>
<a class="sourceLine" id="cb40-5" data-line-number="5">           <span class="kw">with</span>(amostra, n<span class="op">*</span>(<span class="op">-</span>alpha<span class="op">*</span><span class="kw">log</span>(beta)<span class="op">-</span><span class="kw">log</span>(<span class="kw">gamma</span>(alpha)) <span class="op">+</span><span class="st">  </span></a>
<a class="sourceLine" id="cb40-6" data-line-number="6"><span class="st">                </span>(alpha<span class="dv">-1</span>) <span class="op">*</span><span class="st"> </span>media.logs <span class="op">-</span><span class="st"> </span>media<span class="op">/</span>beta))},</a>
<a class="sourceLine" id="cb40-7" data-line-number="7">    <span class="st">&quot;2&quot;</span> =<span class="st"> </span>{alpha &lt;-<span class="st"> </span>par[<span class="dv">1</span>]; lambda &lt;-<span class="st"> </span>par[<span class="dv">2</span>] ;</a>
<a class="sourceLine" id="cb40-8" data-line-number="8">           <span class="kw">with</span>(amostra, n<span class="op">*</span>(alpha<span class="op">*</span><span class="kw">log</span>(lambda)<span class="op">-</span><span class="kw">log</span>(<span class="kw">gamma</span>(alpha)) <span class="op">+</span><span class="st">  </span></a>
<a class="sourceLine" id="cb40-9" data-line-number="9"><span class="st">                </span>(alpha<span class="dv">-1</span>) <span class="op">*</span><span class="st"> </span>media.logs <span class="op">-</span><span class="st"> </span>lambda <span class="op">*</span><span class="st"> </span>media))},</a>
<a class="sourceLine" id="cb40-10" data-line-number="10">    <span class="st">&quot;3&quot;</span> =<span class="st"> </span>{alpha &lt;-<span class="st"> </span>par[<span class="dv">1</span>]; mu &lt;-<span class="st"> </span>par[<span class="dv">2</span>] ;</a>
<a class="sourceLine" id="cb40-11" data-line-number="11">           <span class="kw">with</span>(amostra, n<span class="op">*</span>(alpha<span class="op">*</span>(<span class="kw">log</span>(alpha) <span class="op">-</span><span class="st"> </span><span class="kw">log</span>(mu)) <span class="op">-</span><span class="st"> </span></a>
<a class="sourceLine" id="cb40-12" data-line-number="12"><span class="st">                </span><span class="kw">log</span>(<span class="kw">gamma</span>(alpha)) <span class="op">+</span><span class="st">  </span>(alpha<span class="dv">-1</span>) <span class="op">*</span><span class="st"> </span>media.logs <span class="op">-</span><span class="st"> </span></a>
<a class="sourceLine" id="cb40-13" data-line-number="13"><span class="st">                </span>(alpha<span class="op">/</span>mu) <span class="op">*</span><span class="st"> </span>media))})</a>
<a class="sourceLine" id="cb40-14" data-line-number="14">  <span class="kw">return</span>(<span class="op">-</span>ll)</a>
<a class="sourceLine" id="cb40-15" data-line-number="15">}</a></code></pre></div>
<p>A função em <a href="#lem:Vero-Gama">1.9</a> é escrita em termos de
estatísticas (suficientes) da amostra
para evitar repetições de cálculos e exige que estas quantidades sejam
passadas na forma de uma  pelo argumento .
A função retorna o negativo da verossimilhança.
No exemplo a seguir
começamos então simulando um conjunto de dados com <span class="math inline">\(\alpha=4.5\)</span> e <span class="math inline">\(\beta=2\)</span>,
e criamos o objeto com as estatísticas suficientes.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb41-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">201107</span>)</a>
<a class="sourceLine" id="cb41-2" data-line-number="2">dadosG &lt;-<span class="st"> </span><span class="kw">rgamma</span>(<span class="dv">20</span>, <span class="dt">shape =</span> <span class="fl">4.5</span>, <span class="dt">rate=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb41-3" data-line-number="3">am &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">media=</span><span class="kw">mean</span>(dadosG), <span class="dt">media.logs =</span> <span class="kw">mean</span>(<span class="kw">log</span>(dadosG)), </a>
<a class="sourceLine" id="cb41-4" data-line-number="4">           <span class="dt">n=</span><span class="kw">length</span>(dadosG))</a></code></pre></div>
<p>Quando possível, é mais conveniente fazer o gráfico das superfícies de verossimilhança
na escala da  o que requer o valor máximo da verossimilhança. Assim, obtemos as estimativas de máxima verossimilhança para as 3 parametrizações, usando os códigos abaixo:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb42-1" data-line-number="1">mod1 &lt;-<span class="st"> </span><span class="kw">optim</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), neglLik, <span class="dt">amostra=</span>am, <span class="dt">modelo=</span><span class="dv">1</span>)<span class="op">$</span>par</a>
<a class="sourceLine" id="cb42-2" data-line-number="2">mod2 &lt;-<span class="st"> </span><span class="kw">optim</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), neglLik, <span class="dt">amostra=</span>am, <span class="dt">modelo=</span><span class="dv">2</span>)<span class="op">$</span>par</a>
<a class="sourceLine" id="cb42-3" data-line-number="3">mod3 &lt;-<span class="st"> </span><span class="kw">optim</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), neglLik, <span class="dt">amostra=</span>am, <span class="dt">modelo=</span><span class="dv">3</span>)<span class="op">$</span>par</a>
<a class="sourceLine" id="cb42-4" data-line-number="4"><span class="kw">cbind</span>(mod1, mod2, mod3)</a></code></pre></div>
<pre><code>##           mod1     mod2     mod3
## [1,] 4.6926293 4.693253 4.693953
## [2,] 0.4152213 2.408705 1.948283</code></pre>
<p>Neste ponto vamos simplesmente usar
os objetos ,  e 
que contém as estimativas.
Mais adiante vamos discutir em mais detalhes a obtenção das estimativas.</p>
<p>Os comandos a seguir mostram como obter o gráfico da superfície de
verossimilhança () para a parametrização 1.
Utilizamos a função  genérica
definida em <a href="#lem:dev-generica"><strong>??</strong></a>
para ser usada com densidades com dois parâmetros.
Definimos uma sequencia de valores para cada parâmetro e o valor da 
é calculado em cada ponto da malha que combina os valores usando a função .
A partir da malha os contornos parametrização 1 na escala original dos parâmetros
são desenhados na forma de isolinhas.
Comandos análogos são usados para as demais parametrizações
nas escalas originais e logarítmicas.</p>
<p>Na Figura <a href="#fig:devSurf-Gamma">1.7</a> mostramos as superfícies
para cada parametrização nas escalas originais e logarítmicas dos parâmetros.
Simetria e ortogonalidade da superfície facilitam e melhoram o desempenho de algoritmos numéricos,
sejam de optimização ou de algorítmos de amostragem como por exemplo
os de  (cadeias de Markov por Monte Carlo).
A superfície de verossimilhança para a parametrização 3 é a que apresenta melhores características.
Isto sugere que algorítimos de cálculos de verossimilhança em procedimentos numéricos
utilizando a Gama devem ser
escritos nesta parametrização transformando ao final os resultados para outras
parametrizações de interesse, se for o caso.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb44-1" data-line-number="1">alpha &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">1.5</span>,<span class="fl">10.9</span>, <span class="dt">len=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb44-2" data-line-number="2">beta  &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.17</span>,<span class="fl">1.35</span>, <span class="dt">len=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb44-3" data-line-number="3">dev1  &lt;-<span class="st"> </span><span class="kw">outer</span>(alpha, beta, <span class="dt">FUN =</span> devSurf, <span class="dt">est=</span>mod1, <span class="dt">llFUN=</span>neglLik, <span class="dt">amostra=</span>am, <span class="dt">modelo=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb44-4" data-line-number="4"></a>
<a class="sourceLine" id="cb44-5" data-line-number="5">LEVELS &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.99</span>,<span class="fl">0.95</span>,<span class="fl">0.9</span>,<span class="fl">0.7</span>,<span class="fl">0.5</span>,<span class="fl">0.3</span>,<span class="fl">0.1</span>,<span class="fl">0.05</span>)</a>
<a class="sourceLine" id="cb44-6" data-line-number="6"><span class="kw">contour</span>(alpha,beta, dev1, <span class="dt">levels=</span><span class="kw">qchisq</span>(LEVELS,<span class="dt">df=</span><span class="dv">2</span>),</a>
<a class="sourceLine" id="cb44-7" data-line-number="7">        <span class="dt">labels=</span>LEVELS, <span class="dt">xlab=</span><span class="kw">expression</span>(alpha),<span class="dt">ylab=</span><span class="kw">expression</span>(beta))</a>
<a class="sourceLine" id="cb44-8" data-line-number="8"><span class="kw">points</span>(<span class="kw">t</span>(mod1), <span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">cex=</span><span class="fl">0.6</span>)</a></code></pre></div>
<div class="figure"><span id="fig:devSurf-Gamma"></span>
<img src="Exemplos2_files/figure-html/devSurf-Gamma-1.png" alt="Superfícies de deviance sob diferentes parametrizações - Distribuição Gama." width="864" />
<p class="caption">
Figura 1.7: Superfícies de deviance sob diferentes parametrizações - Distribuição Gama.
</p>
</div>
</div>
</div>
<div id="distribuição-binomial-negativa" class="section level2">
<h2><span class="header-section-number">1.3</span> Distribuição Binomial Negativa</h2>

<div class="example">
<span id="exm:ex7" class="example"><strong>Example 1.4  </strong></span><strong>Exemplo - Distribuição Binomial Negativa</strong>
</div>

<p>Considere <span class="math inline">\(Y_1, Y_2, \ldots, Y_n\)</span> variáveis aleatórias independentes
provenientes de uma distribuição Binomial Negativa de parâmetros <span class="math inline">\(\phi \in \Re^+\)</span> e <span class="math inline">\(0 &lt; p \leq 1\)</span> e <span class="math inline">\(y = 0,1, \ldots\)</span>. Usamos dados de uma amostra <span class="math inline">\(y_1, y_2, \ldots, y_n\)</span> para estimar os parâmetros <span class="math inline">\(\phi\)</span> e <span class="math inline">\(p\)</span> e possivelmente
construir intervalos de confiança e testes de hipóteses. Neste caso temos dois
parâmetros, o que leva a uma superfície de verossimilhança. A função de
distribuição da Binomial Negativa é dada por:</p>
<p><span class="math display">\[
p(y) = \frac{\Gamma(y + \phi)}{\Gamma(\phi) y!} p^\phi (1 - p)^y, \quad \text{para} \quad 0 &lt; p \leq 1 \quad \phi &gt; 0 \quad \text{e} \quad y = 0, 1, 2 , \ldots  .
\]</span></p>
<p>Sendo <span class="math inline">\(n\)</span> amostras independentes a função de verossimilhança tem a seguinte forma,
\begin{flalign<em>}
L(,p) &amp;= <em>{i=1}^n  p^(1 - p)^{y</em>{i}} \
L(,p) &amp;= p^{n} (1-p)<sup>{<em>{i=1}^n y_i} </em>{i=1}</sup>n .
\end{flalign</em>}
Logo a função de log-verossimilhança pode ser escrita como,
<span class="math display">\[
  l(\phi,p) = n \phi \log p + \sum_{i=1}^n y_i \log (1-p) + \sum_{i=1}^n \log \Gamma(y_i + \phi) - n \log \Gamma(\phi) - \sum_{i=1}^n \log y_i !.
\]</span>
Derivando em relação aos parâmetros <span class="math inline">\(\phi\)</span> e <span class="math inline">\(p\)</span> obtém-se as equações escore:
\begin{flalign<em>}
U() &amp;= n p + <em>{i=1}^n (y_i + ) - n () \
U(p) &amp;=  - </em>{i=1}^n y_i / (1-p) ;.
\end{flalign</em>}</p>
<p>A matriz de informação é obtida calculando-se as derivadas:
\begin{flalign<em>}
 =  &amp; =  \ &amp;= <em>{i=1}^n ‘(y_i + ) - n ’() \
 = 
&amp;=  \
&amp;= -  - </em>{i=1}^n y_i /(1-p)^2 \
 =  &amp;= \
&amp;= .
\end{flalign</em>}</p>
<p>Com os resultados pode-se estimar <span class="math inline">\(\phi\)</span> e <span class="math inline">\(p\)</span> através do algoritmo de Newton-Raphson.
Note que pela equação <span class="math inline">\(U(p)\)</span> podemos obter a estimativa de <span class="math inline">\(p\)</span> em função do parâmetro desconhecido <span class="math inline">\(\phi\)</span>
de forma análoga ao que acontece no modelo Gama.
Entretanto, não vamos utiizar a verossimilhança concentrada
para poder exemplificar o uso do algoritmo de Newton-Raphson em duas dimensões.</p>
<p>A implementação do modelo começa sempre com a construção da função de log-verossimilhança, o que fazemos
em código que inicialmente reflete a forma que “escrevemos no papel”.
Adiante veremos como usar as funções residentes do <span class="math inline">\(R\)</span> para implementar um código mais eficiente.
Uma amostra simulada de tamanho <span class="math inline">\(n=100\)</span> e valores dos parâmetros <span class="math inline">\(\phi = 100\)</span> e <span class="math inline">\(p = 0.8\)</span>
fornece os dados utilizados.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb45-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb45-2" data-line-number="2">y &lt;-<span class="st"> </span><span class="kw">rnbinom</span>(<span class="dv">100</span>, <span class="dt">size=</span><span class="dv">100</span>, <span class="dt">p =</span> <span class="fl">0.8</span>)</a></code></pre></div>

<div class="lemma">
<span id="lem:llkl-nb" class="lemma"><strong>Código 1.10  </strong></span><strong>Função de log-verossimilhança para a distribuição binomial negativa.</strong>
</div>

<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb46-1" data-line-number="1">ll.neg.binomial &lt;-<span class="st"> </span><span class="cf">function</span>(par, y){</a>
<a class="sourceLine" id="cb46-2" data-line-number="2">  phi &lt;-<span class="st"> </span>par[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb46-3" data-line-number="3">  p &lt;-<span class="st"> </span>par[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb46-4" data-line-number="4">  n &lt;-<span class="st"> </span><span class="kw">length</span>(y)</a>
<a class="sourceLine" id="cb46-5" data-line-number="5">  ll &lt;-<span class="st"> </span>n<span class="op">*</span>phi<span class="op">*</span><span class="kw">log</span>(p) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(y)<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span>p) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">log</span>(<span class="kw">gamma</span>(y<span class="op">+</span>phi))) <span class="op">-</span><span class="st"> </span></a>
<a class="sourceLine" id="cb46-6" data-line-number="6"><span class="st">        </span>n<span class="op">*</span><span class="kw">log</span>(<span class="kw">gamma</span>(phi)) <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">log</span>(<span class="kw">factorial</span>(y)))</a>
<a class="sourceLine" id="cb46-7" data-line-number="7">  <span class="kw">return</span>(ll)}</a></code></pre></div>
<p>Pensando em otimizar a função de log-verossimilhança através do algoritmo de Newton-Raphson,
o próximo passo é implementar a função escore e, na sequência, a matriz de segundas derivadas ou Hessiana.</p>

<div class="lemma">
<span id="lem:escore-nb" class="lemma"><strong>Código 1.11  </strong></span><strong>Funções escore e hessiana para a distribuição binomial negativa.</strong>
</div>

<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb47-1" data-line-number="1">escore &lt;-<span class="st"> </span><span class="cf">function</span>(par, y){</a>
<a class="sourceLine" id="cb47-2" data-line-number="2">  phi &lt;-<span class="st"> </span>par[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb47-3" data-line-number="3">  p &lt;-<span class="st"> </span>par[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb47-4" data-line-number="4">  n &lt;-<span class="st"> </span><span class="kw">length</span>(y)</a>
<a class="sourceLine" id="cb47-5" data-line-number="5">  U.phi &lt;-<span class="st"> </span>n<span class="op">*</span><span class="kw">log</span>(p) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">digamma</span>(y<span class="op">+</span>phi)) <span class="op">-</span><span class="st"> </span>n<span class="op">*</span><span class="kw">digamma</span>(phi)</a>
<a class="sourceLine" id="cb47-6" data-line-number="6">  U.p &lt;-<span class="st"> </span>(n<span class="op">*</span>phi)<span class="op">/</span>p <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(y)<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>p)</a>
<a class="sourceLine" id="cb47-7" data-line-number="7">  <span class="kw">return</span>(<span class="kw">c</span>(U.phi,U.p))}</a>
<a class="sourceLine" id="cb47-8" data-line-number="8">Hessiana &lt;-<span class="st"> </span><span class="cf">function</span>(par, y){</a>
<a class="sourceLine" id="cb47-9" data-line-number="9">  phi =<span class="st"> </span>par[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb47-10" data-line-number="10">  p =<span class="st"> </span>par[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb47-11" data-line-number="11">  n &lt;-<span class="st"> </span><span class="kw">length</span>(y)</a>
<a class="sourceLine" id="cb47-12" data-line-number="12">  II &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">sum</span>(<span class="kw">trigamma</span>(y<span class="op">+</span>phi)) <span class="op">-</span><span class="st"> </span>n<span class="op">*</span><span class="kw">trigamma</span>(phi), </a>
<a class="sourceLine" id="cb47-13" data-line-number="13">               n<span class="op">/</span>p,n<span class="op">/</span>p, <span class="op">-</span>(n<span class="op">*</span>phi)<span class="op">/</span>p<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(y)<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>p)<span class="op">^</span><span class="dv">2</span>),<span class="dv">2</span>,<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb47-14" data-line-number="14">  <span class="kw">return</span>(II)}</a></code></pre></div>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb48-1" data-line-number="1"><span class="kw">NewtonRaphson</span>(<span class="dt">initial=</span><span class="kw">c</span>(<span class="dv">50</span>,<span class="fl">0.5</span>), <span class="dt">escore=</span>escore,<span class="dt">hessiano=</span>Hessiana,</a>
<a class="sourceLine" id="cb48-2" data-line-number="2">              <span class="dt">max.iter=</span><span class="dv">100</span>, <span class="dt">tol =</span> <span class="fl">1e-10</span>, <span class="dt">n.dim=</span><span class="dv">2</span>, <span class="dt">y=</span>y)</a></code></pre></div>
<pre><code>## [1] 114.1976249   0.8213433</code></pre>
<p>Para construção do intervalo de confiança assintótico e/ou baseado em perfil de verossimilhança podemos proceder exatamente igual ao exemplo da distribuição Gama. Deixamos como exercício para o leitor obter estes intervalos e compará-los.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
